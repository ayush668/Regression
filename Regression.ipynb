{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e88ab54-12a5-473e-929d-348f65624202",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                       # Regrssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a7b77-3435-47e9-ba04-a4d66441d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. What is Simple Linear Regression.\n",
    "\"\"\"\n",
    "Simple Linear Regression\n",
    "\n",
    "Simple Linear Regression is a statistical method used to model the relationship between two variables:\n",
    "1. **Independent Variable (X)** – The predictor or explanatory variable.\n",
    "2. **Dependent Variable (Y)** – The response or outcome variable.\n",
    "\n",
    "It assumes a linear relationship between these two variables and fits a straight-line equation:\n",
    "\n",
    "    Y = β0 + β1X + ε\n",
    "\n",
    "where:\n",
    "- Y = Dependent variable (the variable we want to predict)\n",
    "- X = Independent variable (the predictor)\n",
    "- β0 = Intercept (the value of Y when X = 0)\n",
    "- β1 = Slope (the change in Y for a one-unit increase in X)\n",
    "- ε = Error term (captures random variations not explained by X)\n",
    "\n",
    "Assumptions of Simple Linear Regression:\n",
    "1. **Linearity** – The relationship between X and Y is linear.\n",
    "2. **Independence** – Observations are independent of each other.\n",
    "3. **Homoscedasticity** – The variance of residuals (errors) is constant across all values of X.\n",
    "4. **Normality** – The residuals follow a normal distribution.\n",
    "\n",
    "How Simple Linear Regression Works:\n",
    "1. **Data Collection** – Collect data with one dependent and one independent variable.\n",
    "2. **Model Training** – Fit a straight-line equation using the least squares method, which minimizes the sum of squared differences between actual and predicted values.\n",
    "3. **Model Evaluation** – Use statistical metrics like Mean Squared Error (MSE) and R-squared (R²) to measure the model’s accuracy.\n",
    "4. **Prediction** – Use the trained model to predict Y for new values of X.\n",
    "\n",
    "Example Use Cases:\n",
    "- **Predicting Sales** – Based on advertising spend.\n",
    "- **House Price Estimation** – Using square footage as a predictor.\n",
    "- **Student Performance Analysis** – Relationship between study hours and exam scores.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437400b3-8de8-4eab-9c2d-aa5a2c54b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2- What are the key assumptions of Simple Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2437927-7a01-4b18-aa31-e6cc971a2592",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Key Assumptions of Simple Linear Regression\n",
    "Linearity – The relationship between the independent variable (X) and the dependent variable (Y) is linear. This means that changes in X are proportional to changes in Y.\n",
    "\n",
    "Independence – The observations in the dataset are independent of each other. This ensures that the relationship is not influenced by external dependencies.\n",
    "\n",
    "Homoscedasticity – The variance of the residuals (errors) should remain constant across all values of X. If the variance of residuals changes, it may indicate heteroscedasticity, which can lead to unreliable predictions.\n",
    "\n",
    "Normality of Residuals – The residuals (differences between actual and predicted values) should follow a normal distribution. This assumption is crucial for hypothesis testing and confidence intervals.\n",
    "\n",
    "No Multicollinearity – In simple linear regression, there is only one independent variable, so this assumption is automatically satisfied. However, in multiple linear regression, independent variables should not be highly correlated.\n",
    "\n",
    "No Autocorrelation – The residuals should not be correlated with each other. This is especially important for time-series data, where autocorrelation can affect the model’s accuracy.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4011da-601d-4602-b4ff-5fb451f156ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.What does the coefficient m represent in the equation Y=mX+c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52759573-7fb3-4ad0-8a42-f2ec2e62bc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In the context of  regression—the equation\n",
    "Y = mX + c\n",
    "\n",
    "is used to model the relationship between an input feature \\( X \\) and the output \\( Y \\). Here’s what the coefficient \\( m \\) represents:\n",
    "\n",
    "- **Slope/Weight:**  \n",
    "  \\( m \\) is the **slope** of the line, which indicates how much \\( Y \\) changes for each one-unit change in \\( X \\). In other words, it shows the \n",
    "  strength and direction of the relationship between the input \\( X \\) and the predicted output \\( Y \\).\n",
    "\n",
    "- **Learning Parameter:**  \n",
    "  In machine learning, \\( m \\) is a parameter that the algorithm learns during training. The goal of training is to adjust \\( m \\) (and \\( c \\)) so \n",
    "  that the predictions \\( Y \\) closely match the actual data.\n",
    "\n",
    "### Example:\n",
    "If you are predicting house prices based on the size of the house:\n",
    "- \\( X \\) represents the size in square feet.\n",
    "- \\( Y \\) represents the house price.\n",
    "- If \\( m = 150 \\), then for every additional square foot, the predicted house price increases by \\$150.\n",
    "- \\( c \\) represents the base price when the house size is zero.\n",
    "\n",
    "In summary, in linear regression, the coefficient \\( m \\) tells you how much impact a change in the input variable \\( X \\) has on the output \\( Y \\).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559fb2b-aa6a-489a-8ba3-dd5a59eecb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.What does the intercept c represent in the equation Y=mX+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296388c0-931f-4f4f-8981-17442724790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In **linear regression**, the equation\n",
    "\n",
    "```\n",
    "Y = mX + c\n",
    "```\n",
    "\n",
    "models the relationship between an input variable \\(X\\) and the output \\(Y\\). Here’s what the intercept \\(c\\) represents:\n",
    "\n",
    "- **Intercept/Bias:**  \n",
    "  \\(c\\) is the **intercept** (often also called the **bias**). It represents the predicted value of \\(Y\\) when \\(X\\) is zero. In other words, it is the starting value or baseline level of the output before considering the effect of \\(X\\).\n",
    "\n",
    "- **Role in the Model:**  \n",
    "  Including \\(c\\) allows the regression line to be shifted vertically to best fit the data. Without \\(c\\), the line would be forced to pass through the origin (0,0), which might not be an accurate representation of the relationship between \\(X\\) and \\(Y\\).\n",
    "\n",
    "- **Learning the Intercept:**  \n",
    "  Just like the coefficient \\(m\\), \\(c\\) is a parameter that the learning algorithm estimates during training to minimize the error between the predicted and actual values.\n",
    "\n",
    "### Example:\n",
    "Consider predicting house prices based on the size of the house:\n",
    "- \\(X\\) = Size of the house (in square feet)\n",
    "- \\(Y\\) = Predicted house price\n",
    "- \\(m = 150\\) indicates that for each additional square foot, the price increases by \\$150.\n",
    "- \\(c = 20,\\!000\\) indicates that when the size is 0, the base price is \\$20,000.\n",
    "\n",
    "The equation would be:\n",
    "\n",
    "```\n",
    "Y = 150 * X + 20,000\n",
    "```\n",
    "\n",
    "This means if a house had 0 square feet (hypothetically), the model would start at a baseline price of \\$20,000, and then add \\$150 for\n",
    "each additional square foot.\n",
    "\n",
    "In summary, in linear regression, the intercept \\(c\\) sets the baseline prediction for \\(Y\\) when \\(X\\) is zero, allowing the model to more \n",
    "accurately capture the underlying trend in the data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7491d-0e01-41d9-8991-a255dad0d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5- How do we calculate the slope m in Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245a03d-054d-406e-b23c-27f4218de59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "In Simple Linear Regression, the slope m is calculated using the least squares method, which minimizes the sum of the squared differences between\n",
    "the actual values and the predicted values from the model.\n",
    "\n",
    "Given a dataset with data points (X1, Y1), (X2, Y2), …, (Xn, Yn), the slope m is computed as:\n",
    "\n",
    "    m = Σ((Xi - X̄) * (Yi - Ȳ)) / Σ((Xi - X̄)^2)\n",
    "\n",
    "Where:\n",
    "- X̄ is the mean (average) of the X values.\n",
    "- Ȳ is the mean (average) of the Y values.\n",
    "- The summations (Σ) run over all data points from i = 1 to n.\n",
    "\n",
    "Step-by-step:\n",
    "\n",
    "1. Compute the Mean of X and Y:\n",
    "   - X̄ = (1/n) * Σ(Xi)\n",
    "   - Ȳ = (1/n) * Σ(Yi)\n",
    "\n",
    "2. Calculate the Numerator:\n",
    "   - For each data point, compute the differences (Xi - X̄) and (Yi - Ȳ).\n",
    "   - Multiply these differences: (Xi - X̄) * (Yi - Ȳ) for each i.\n",
    "   - Sum all these products: Σ((Xi - X̄) * (Yi - Ȳ)).\n",
    "\n",
    "3. Calculate the Denominator:\n",
    "   - For each data point, compute (Xi - X̄)^2.\n",
    "   - Sum these squared differences: Σ((Xi - X̄)^2).\n",
    "\n",
    "4. Divide the Numerator by the Denominator:\n",
    "   - m = Σ((Xi - X̄) * (Yi - Ȳ)) / Σ((Xi - X̄)^2)\n",
    "\n",
    "This slope m represents the rate at which Y changes for a one-unit change in X, thereby capturing the linear relationship between X and Y.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba37d8-11c4-4a39-ad69-0175f51203f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 What is the purpose of the least squares method in Simple Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ec9ab-6f00-4bdd-b748-1ab20cfa4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "The least squares method is a mathematical approach used to determine the best-fitting line through a set of data points. Its primary purpose is to\n",
    "minimize the sum of the squared differences (errors) between the actual observed values and the values predicted by the linear model.\n",
    "\n",
    "Key objectives of the least squares method include:\n",
    "\n",
    "1. **Minimizing Error:**  \n",
    "   It minimizes the total squared error (residuals) between the actual data points and the predictions of the model, ensuring the best possible fit.\n",
    "\n",
    "2. **Optimal Parameter Estimation:**  \n",
    "   By reducing the overall error, the method computes the optimal values of the slope (m) and the intercept (c). These parameters define the line that \n",
    "   best represents the relationship between the independent variable (X) and the dependent variable (Y).\n",
    "\n",
    "3. **Analytical Simplicity:**  \n",
    "   The least squares method provides closed-form solutions for the model parameters, which makes it computationally efficient and straightforward to \n",
    "   implement.\n",
    "\n",
    "4. **Improved Predictive Accuracy:**  \n",
    "   A model with minimized error typically makes more accurate predictions on new, unseen data, as it closely captures the underlying trend of the data.\n",
    "\n",
    "In summary, the least squares method is essential in Simple Linear Regression because it systematically identifies the line that minimizes the \n",
    "prediction error, leading to the most accurate representation of the relationship between X and Y.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b346dca-c312-49ff-98f9-208355e51cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
    "'''\n",
    "The coefficient of determination, denoted as R², measures how well the independent variable X explains the variability of the dependent variable Y.\n",
    "In Simple Linear Regression, R² is interpreted as follows:\n",
    "\n",
    "1. **Explained Variance:**  \n",
    "   R² represents the proportion of the total variance in Y that is explained by the model using X. For example, an R² value of 0.80 means that 80% of\n",
    "   the variability in Y is accounted for by X, while the remaining 20% is due to factors not captured by the model or random noise.\n",
    "\n",
    "2. **Goodness-of-Fit:**  \n",
    "   A higher R² indicates a better fit of the regression line to the data. When R² is close to 1, the model explains most of the variability in Y; \n",
    "   when it is close to 0, the model explains very little of the variability.\n",
    "\n",
    "3. **Interpretation Range:**  \n",
    "   R² values range from 0 to 1:\n",
    "     - **R² = 0:** The model does not explain any of the variability in Y.\n",
    "     - **R² = 1:** The model perfectly explains the variability in Y.\n",
    "     - Values between 0 and 1 indicate the fraction of the variance in Y that is explained by X.\n",
    "\n",
    "4. **Limitations:**  \n",
    "   - R² does not imply causation; it only measures the strength of the association.\n",
    "   - A high R² does not necessarily mean the model is appropriate—it could be the result of overfitting, especially in more complex models.\n",
    "   - In cases with multiple predictors, adjusted R² is often used to account for the number of variables in the model.\n",
    "\n",
    "In summary, in Simple Linear Regression, R² quantifies the extent to which the independent variable X explains the variation in the dependent variable Y,\n",
    "providing a measure of the model's goodness-of-fit.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c6247-af52-47f7-9f42-97fd6d0b6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 What is Multiple Linear Regression.\n",
    "'''\n",
    "Multiple Linear Regression is an extension of Simple Linear Regression that models the relationship between one dependent variable (Y) and two or more independent variables (X1, X2, ..., Xn). Instead of fitting a straight line through data points, Multiple Linear Regression fits a hyperplane in multidimensional space.\n",
    "\n",
    "The general equation for Multiple Linear Regression is:\n",
    "\n",
    "    Y = m1 * X1 + m2 * X2 + ... + mn * Xn + c\n",
    "\n",
    "Where:\n",
    "- Y is the dependent variable (the output you want to predict).\n",
    "- X1, X2, ..., Xn are the independent variables (the inputs or predictors).\n",
    "- m1, m2, ..., mn are the coefficients (slopes) that represent the change in Y for a one-unit change in the corresponding independent variable, holding all other variables constant.\n",
    "- c is the intercept (bias), representing the predicted value of Y when all the independent variables are zero.\n",
    "\n",
    "Purpose and Use:\n",
    "1. **Modeling Complex Relationships:**  \n",
    "   Multiple Linear Regression allows us to analyze and predict the behavior of Y based on several factors at once.\n",
    "   \n",
    "2. **Improved Predictive Accuracy:**  \n",
    "   By incorporating more predictors, the model can capture a wider range of influences on Y, often leading to more accurate predictions.\n",
    "   \n",
    "3. **Understanding Variable Impact:**  \n",
    "   The coefficients (m1, m2, ..., mn) help quantify the impact of each independent variable on the dependent variable, making it possible to assess which factors are most influential.\n",
    "\n",
    "Assumptions:\n",
    "Multiple Linear Regression relies on several assumptions for the model to be valid:\n",
    "- **Linearity:** The relationship between each independent variable and the dependent variable is linear.\n",
    "- **Independence:** Observations (and residuals) are independent of each other.\n",
    "- **Homoscedasticity:** The variance of residuals is constant across all levels of the independent variables.\n",
    "- **Normality:** The residuals of the model are approximately normally distributed.\n",
    "- **No Multicollinearity:** The independent variables are not highly correlated with each other, which can distort the estimation of coefficients.\n",
    "\n",
    "Overall, Multiple Linear Regression is a fundamental statistical and machine learning technique used for predicting outcomes and understanding the influence of multiple variables on a single response variable.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e1e8e-5905-432a-8922-624eec09528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.What is the main difference between Simple and Multiple Linear Regression\n",
    "'''\n",
    "The primary difference between Simple and Multiple Linear Regression is the number of independent variables (predictors) used to explain the \n",
    "variation in the dependent variable (Y).\n",
    "\n",
    "1. Number of Predictors:\n",
    "   - Simple Linear Regression:\n",
    "     • Uses a single predictor variable (X) to predict the outcome (Y).\n",
    "     • Example: Predicting a student's test score (Y) based on the number of study hours (X).\n",
    "   \n",
    "   - Multiple Linear Regression:\n",
    "     • Uses two or more predictor variables (X1, X2, ..., Xn) to predict the outcome (Y).\n",
    "     • Example: Predicting a student's test score (Y) based on the number of study hours (X1), attendance rate (X2), and previous GPA (X3).\n",
    "\n",
    "2. Model Representation:\n",
    "   - Simple Linear Regression:\n",
    "     • The relationship is modeled with a straight line in a two-dimensional space.\n",
    "     • Equation: Y = m * X + c\n",
    "       - 'm' is the slope, representing the change in Y for a one-unit change in X.\n",
    "       - 'c' is the intercept, the value of Y when X is zero.\n",
    "   \n",
    "   - Multiple Linear Regression:\n",
    "     • The relationship is modeled as a hyperplane in multidimensional space.\n",
    "     • Equation: Y = m1 * X1 + m2 * X2 + ... + mn * Xn + c\n",
    "       - Each 'mi' is the coefficient corresponding to predictor Xi, indicating the change in Y for a one-unit change in Xi while holding all\n",
    "       other variables constant.\n",
    "       - 'c' is the intercept, the value of Y when all predictors are zero.\n",
    "\n",
    "3. Interpretation of Coefficients:\n",
    "   - Simple Linear Regression:\n",
    "     • The single coefficient 'm' directly quantifies the impact of X on Y.\n",
    "   \n",
    "   - Multiple Linear Regression:\n",
    "     • Each coefficient 'mi' represents the unique contribution of its corresponding predictor Xi to the outcome Y, controlling for the effects of \n",
    "     other predictors.\n",
    "     • This allows for a more nuanced understanding of how multiple factors simultaneously influence Y.\n",
    "\n",
    "4. Assumptions and Considerations:\n",
    "   - Both methods assume a linear relationship between predictors and the dependent variable.\n",
    "   - In Multiple Linear Regression, there is an additional need to check for multicollinearity, which occurs when predictor variables are highly\n",
    "   correlated with each other. High multicollinearity can distort the estimation of the coefficients.\n",
    "\n",
    "5. Application:\n",
    "   - Simple Linear Regression is best used when there is one key predictor for the outcome.\n",
    "   - Multiple Linear Regression is useful when the outcome is influenced by several factors, enabling a more comprehensive model that may improve\n",
    "   predictive accuracy.\n",
    "\n",
    "In summary, while Simple Linear Regression models the relationship between one predictor and the outcome, Multiple Linear Regression extends this\n",
    "concept to incorporate multiple predictors, providing a more detailed analysis of the factors that affect the dependent variable.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37833c9-ff0c-4607-9a46-18e63ae5ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 What are the key assumptions of Multiple Linear Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03a3c8-6353-4c18-9850-675612166f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Multiple Linear Regression relies on several key assumptions to ensure that the model’s estimates are reliable and that any\n",
    "inferences made are valid. These assumptions include:\n",
    "\n",
    "1. **Linearity:**\n",
    "   - The relationship between the dependent variable (Y) and each independent variable (X1, X2, ..., Xn) is linear. This means that a change in any \n",
    "   predictor leads to a proportional change in Y.\n",
    "\n",
    "2. **Independence:**\n",
    "   - The observations in the dataset are independent of each other. This also implies that the residuals (errors) are independent; there should be no \n",
    "   correlation between the errors of different observations.\n",
    "\n",
    "3. **Homoscedasticity:**\n",
    "   - The variance of the residuals should be constant across all levels of the independent variables. In other words, the spread of the errors should be\n",
    "   roughly the same for all predicted values of Y.\n",
    "\n",
    "4. **Normality of Residuals:**\n",
    "   - The residuals should be approximately normally distributed. This assumption is particularly important for hypothesis testing and constructing \n",
    "   confidence intervals for the model parameters.\n",
    "\n",
    "5. **No (or Little) Multicollinearity:**\n",
    "   - The independent variables should not be highly correlated with each other. High multicollinearity can make it difficult to isolate the individual \n",
    "   effect of each predictor on Y and can lead to unstable estimates of the coefficients.\n",
    "\n",
    "6. **No Autocorrelation:**\n",
    "   - Particularly important for time series data, this assumption states that residuals should not be correlated with each other. Autocorrelation \n",
    "   (correlated errors) violates the independence assumption and can lead to biased standard errors.\n",
    "\n",
    "7. **Correct Model Specification:**\n",
    "   - The model should include all relevant predictors and exclude irrelevant ones. Omitting key variables or including unnecessary ones can bias the \n",
    "   coefficient estimates and impair the model's predictive power.\n",
    "\n",
    "Verifying these assumptions is a crucial step in model validation. Diagnostic plots (such as residual plots and Q-Q plots) and statistical tests \n",
    "(like the Durbin-Watson test for autocorrelation or the Variance Inflation Factor for multicollinearity) are commonly used to assess whether these \n",
    "assumptions hold in a given dataset.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6542ac39-011a-4d9b-976d-d19c54b2b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11 What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
    "'''\n",
    "**Heteroscedasticity** refers to the condition in which the variance of the residuals (errors) is not constant across all levels of the independent \n",
    "variables. In simpler terms, the spread or dispersion of the residuals changes as the value of the independent variables changes. This violates one \n",
    "of the key assumptions of Multiple Linear Regression, which assumes **homoscedasticity**—that the variance of the residuals remains constant.\n",
    "\n",
    "### How to Identify Heteroscedasticity:\n",
    "- **Residual Plots:**  \n",
    "  A common way to detect heteroscedasticity is by plotting the residuals against the fitted (predicted) values of the dependent variable or the \n",
    "  independent variables. If the spread of residuals increases or decreases as the predicted values change, this indicates heteroscedasticity.\n",
    "  \n",
    "- **Breusch-Pagan Test or White Test:**  \n",
    "  These are statistical tests specifically designed to detect heteroscedasticity.\n",
    "\n",
    "### Effects of Heteroscedasticity on Multiple Linear Regression:\n",
    "1. **Inefficient Estimation of Coefficients:**\n",
    "   - While heteroscedasticity does not bias the coefficient estimates (the values of the regression coefficients remain unbiased), it affects the\n",
    "   **efficiency** of the estimates. This means that the standard errors of the coefficients are likely to be inaccurate, leading to less reliable hypothesis tests (e.g., t-tests) and confidence intervals.\n",
    "\n",
    "2. **Inflated Type I and Type II Errors:**\n",
    "   - In the presence of heteroscedasticity, the standard errors of the regression coefficients may be too small or too large. This can lead to incorrect conclusions about the significance of the predictors. For instance:\n",
    "     - **Type I Error (False Positive):** You may incorrectly conclude that a predictor is significant when it is not.\n",
    "     - **Type II Error (False Negative):** You may incorrectly fail to detect a significant predictor.\n",
    "\n",
    "3. **Misleading Goodness-of-Fit Measures:**\n",
    "   - The **R-squared** value (coefficient of determination) and other goodness-of-fit metrics may become less reliable when heteroscedasticity is \n",
    "   present because they rely on constant variance for their accuracy.\n",
    "\n",
    "### How to Address Heteroscedasticity:\n",
    "1. **Transformations:**\n",
    "   - You can try transforming the dependent variable (Y) using a log, square root, or other transformations. This can sometimes stabilize the variance \n",
    "   of residuals.\n",
    "\n",
    "2. **Weighted Least Squares (WLS):**\n",
    "   - In cases of heteroscedasticity, a weighted least squares regression can be used. This method assigns different weights to data points based on the \n",
    "   variance of the residuals.\n",
    "\n",
    "3. **Robust Standard Errors:**\n",
    "   - Another approach is to use robust standard errors, which are adjusted to account for heteroscedasticity. This allows for more reliable hypothesis \n",
    "   testing even in the presence of non-constant variance.\n",
    "\n",
    "In summary, heteroscedasticity does not affect the unbiasedness of the regression coefficients but makes them less efficient and distorts statistical\n",
    "tests, leading to potentially incorrect conclusions. It’s important to test for and address heteroscedasticity to ensure valid inferences from the model.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5cdb6-7336-4bb0-86e0-fc006f98d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12 How can you improve a Multiple Linear Regression model with high multicollinearity.\n",
    "'''\n",
    "\n",
    "**Multicollinearity** occurs when two or more independent variables in a Multiple Linear Regression model are highly correlated with each other. This can make it difficult to assess the individual effect of each predictor on the dependent variable, leading to unreliable estimates of the coefficients and inflated standard errors.\n",
    "\n",
    "### How Multicollinearity Affects the Model:\n",
    "1. **Unstable Coefficients:**  \n",
    "   Multicollinearity makes the coefficients of the correlated variables highly sensitive to changes in the data, leading to large variations in the estimated values.\n",
    "   \n",
    "2. **Inflated Standard Errors:**  \n",
    "   The standard errors of the estimated coefficients increase, which can make it difficult to determine whether a predictor is statistically significant. This increases the risk of Type II errors (failing to reject the null hypothesis when it is false).\n",
    "\n",
    "3. **Reduced Interpretability:**  \n",
    "   Since the independent variables are correlated, it becomes hard to interpret the relationship between each predictor and the dependent variable, as their effects are confounded.\n",
    "\n",
    "### Ways to Address High Multicollinearity:\n",
    "\n",
    "1. **Remove Highly Correlated Predictors:**\n",
    "   - One of the simplest solutions is to remove one of the highly correlated predictors. If two variables are strongly correlated, one of them may be redundant, and eliminating it can improve the model's stability.\n",
    "\n",
    "2. **Combine Correlated Variables:**\n",
    "   - If multiple variables are measuring the same underlying concept, you can combine them into a single predictor using techniques like **Principal Component Analysis (PCA)** or **Factor Analysis**. These methods reduce the dimensionality of the data by creating a smaller set of uncorrelated variables (principal components).\n",
    "\n",
    "3. **Use Regularization (Ridge or Lasso Regression):**\n",
    "   - **Ridge Regression (L2 Regularization):** Adds a penalty to the sum of the squared coefficients, shrinking them towards zero. This can help mitigate the effects of multicollinearity by reducing the impact of highly correlated predictors.\n",
    "   - **Lasso Regression (L1 Regularization):** Adds a penalty to the sum of the absolute values of the coefficients. Lasso tends to shrink the coefficients of less important predictors to zero, effectively performing feature selection.\n",
    "   \n",
    "   Both of these methods can help improve model performance in the presence of multicollinearity by reducing overfitting and improving coefficient stability.\n",
    "\n",
    "4. **Increase Sample Size:**\n",
    "   - If feasible, increasing the sample size can reduce the effects of multicollinearity. With more data, the model has a better chance of distinguishing the individual effects of the correlated variables.\n",
    "\n",
    "5. **Examine Correlation Matrix or VIF (Variance Inflation Factor):**\n",
    "   - **Correlation Matrix:** A correlation matrix can help identify pairs of predictors that are highly correlated. If two variables are very close to each other in terms of correlation (e.g., > 0.8 or <-0.8), you may consider removing one.\n",
    "   - **VIF (Variance Inflation Factor):** VIF quantifies how much the variance of the estimated regression coefficients is inflated due to collinearity with other predictors. If the VIF of a predictor is greater than 5 or 10, it indicates high multicollinearity, and the predictor may need to be removed or adjusted.\n",
    "\n",
    "6. **Transform the Variables:**\n",
    "   - Sometimes, applying transformations to the predictors (e.g., log transformation, polynomial terms) can reduce multicollinearity by altering the relationship between variables.\n",
    "\n",
    "### Conclusion:\n",
    "Multicollinearity can significantly affect the stability and interpretability of a Multiple Linear Regression model. By removing highly correlated predictors, combining variables, using regularization techniques, increasing sample size, or utilizing diagnostic tools like VIF, you can improve the model and make the regression results more reliable and interpretable.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c42ad1-14c5-45fa-857b-ed0e765496da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13. What are some common techniques for transforming categorical variables for use in regression models\n",
    "'''\n",
    "Categorical variables, which represent categories or groups, need to be transformed into numerical values before they can be used in regression models. \n",
    "Here are some common techniques for transforming categorical variables for use in regression models:\n",
    "\n",
    "1. **One-Hot Encoding:**\n",
    "   - One of the most common techniques for transforming categorical variables is one-hot encoding. In this method, each category is converted into a \n",
    "   separate binary (0 or 1) column, indicating whether the observation belongs to that category.\n",
    "   - **Example:**\n",
    "     - If a \"Color\" variable has categories \"Red\", \"Blue\", and \"Green\", one-hot encoding would create three columns: \"Color_Red\", \"Color_Blue\", and \n",
    "     \"Color_Green\", with values of 1 or 0 depending on the category of each observation.\n",
    "   \n",
    "   - **When to Use:**  \n",
    "     One-hot encoding is ideal when the categorical variable is nominal (no inherent order or ranking between categories) and when there are a \n",
    "     relatively small number of categories.\n",
    "\n",
    "2. **Label Encoding:**\n",
    "   - Label encoding assigns a unique integer to each category. It is particularly useful for ordinal categorical variables, where the categories have \n",
    "   a meaningful order (e.g., \"Low\", \"Medium\", \"High\").\n",
    "   - **Example:**\n",
    "     - For a \"Size\" variable with categories \"Small\", \"Medium\", and \"Large\", label encoding would assign the values: \"Small\" = 0, \"Medium\" = 1, \"Large\" \n",
    "     = 2.\n",
    "   \n",
    "   - **When to Use:**  \n",
    "     Label encoding is suitable when the categorical variable has a natural order or ranking (ordinal data). However, it should not be used for nominal \n",
    "     variables, as it can unintentionally introduce a notion of hierarchy between categories that doesn't exist.\n",
    "\n",
    "3. **Ordinal Encoding:**\n",
    "   - A variant of label encoding, ordinal encoding explicitly takes into account the order of categories by assigning integers that reflect the ordinal \n",
    "   nature of the categories.\n",
    "   - **Example:**\n",
    "     - For an ordinal variable like \"Education Level\" with categories \"High School\", \"Undergraduate\", and \"Graduate\", ordinal encoding could assign \n",
    "     values such as \"High School\" = 0, \"Undergraduate\" = 1, \"Graduate\" = 2.\n",
    "   \n",
    "   - **When to Use:**  \n",
    "     Ordinal encoding is appropriate when the categories have a clear ordering but no specific interval between them.\n",
    "\n",
    "4. **Binary Encoding:**\n",
    "   - Binary encoding is used when a categorical variable has a large number of categories. It converts each category into a binary number, and each \n",
    "   binary digit is represented by a new column.\n",
    "   - **Example:**\n",
    "     - If a \"Country\" variable has 8 categories, binary encoding would convert each category into a binary number (e.g., Country 1 = 000, Country 2 = \n",
    "     001, Country 3 = 010, etc.) and represent these values across several new columns.\n",
    "   \n",
    "   - **When to Use:**  \n",
    "     Binary encoding is useful when there are many categories, as it reduces the dimensionality compared to one-hot encoding.\n",
    "\n",
    "5. **Frequency or Count Encoding:**\n",
    "   - In frequency encoding, categories are replaced with the count (or frequency) of their occurrence in the dataset. Similarly, count encoding assigns\n",
    "   each category a numeric value based on its frequency in the data.\n",
    "   - **Example:**\n",
    "     - If the category \"Country\" has values like \"USA\" (count = 3), \"Canada\" (count = 2), and \"Mexico\" (count = 1), frequency encoding would replace \n",
    "     these categories with their corresponding counts: \"USA\" = 3, \"Canada\" = 2, \"Mexico\" = 1.\n",
    "   \n",
    "   - **When to Use:**  \n",
    "     Frequency encoding is helpful when the number of occurrences of a category might carry predictive power and can be used for ordinal or nominal \n",
    "     data.\n",
    "\n",
    "6. **Target Encoding (Mean Encoding):**\n",
    "   - Target encoding involves replacing each category with the mean of the target variable (dependent variable) for that category.\n",
    "   - **Example:**\n",
    "     - If you are predicting house prices and the \"Neighborhood\" variable has categories \"A\", \"B\", and \"C\", target encoding would replace each category \n",
    "     with the average house price for that neighborhood.\n",
    "   \n",
    "   - **When to Use:**  \n",
    "     Target encoding is useful when there is a relationship between the categorical variable and the dependent variable and can be particularly \n",
    "     powerful for variables with many categories.\n",
    "\n",
    "7. **Custom Binary Encoding (for Custom Data):**\n",
    "   - In some cases, you may want to create your own encoding scheme based on the business logic or domain knowledge of the problem. For instance, \n",
    "   you could create binary features based on the presence or absence of certain conditions (e.g., a \"Premium\" customer classification).\n",
    "\n",
    "### When to Choose Which Technique?\n",
    "- **One-Hot Encoding** is usually the best choice when dealing with nominal categories with no inherent order and a manageable number of categories.\n",
    "- **Label Encoding** and **Ordinal Encoding** are suitable for ordinal data, where there is a meaningful order between the categories.\n",
    "- **Binary Encoding** and **Frequency Encoding** are appropriate when the categorical variable has a large number of categories and one-hot encoding \n",
    "would result in too many new columns.\n",
    "- **Target Encoding** is useful when the categorical variable directly influences the dependent variable, especially when categories are numerous.\n",
    "\n",
    "In summary, the appropriate technique for transforming categorical variables depends on the nature of the categorical variable (nominal vs. ordinal) \n",
    "and the specific characteristics of your dataset. Choosing the right encoding method can significantly impact the performance of your regression model.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f62e0-ffb7-4f0c-874f-88b8fe766e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14.What is the role of interaction terms in Multiple Linear Regression\n",
    "'''\n",
    "**Interaction terms** in Multiple Linear Regression represent the combined effect of two or more predictor variables on the dependent variable (Y) that cannot be captured by simply adding their individual effects. Including interaction terms allows the model to account for situations where the effect of one predictor on the outcome depends on the value of another predictor.\n",
    "\n",
    "### The Role of Interaction Terms:\n",
    "1. **Capturing Synergistic Effects:**\n",
    "   - Interaction terms help model the joint effect of two or more independent variables. Sometimes, the effect of one predictor on the dependent variable is not constant but changes depending on the level of another predictor.\n",
    "   - **Example:**\n",
    "     - In a study predicting house prices (Y), the interaction between \"Square Footage\" (X1) and \"Number of Bedrooms\" (X2) might be important. The value of square footage may have a stronger impact on price if the house has more bedrooms, but less impact if the house has fewer bedrooms.\n",
    "\n",
    "2. **Improving Model Accuracy:**\n",
    "   - By including interaction terms, the model can better fit the data and provide more accurate predictions, especially when there are complex relationships between predictors that simple additive models cannot capture.\n",
    "\n",
    "3. **Changing the Interpretation of Main Effects:**\n",
    "   - The inclusion of interaction terms changes the interpretation of the main effects (the individual predictors). For example, the effect of \"X1\" on \"Y\" is no longer constant but depends on the value of the interacting predictor \"X2.\"\n",
    "   - **Example:**\n",
    "     - If you have an interaction term between \"X1\" and \"X2\" (X1 * X2), the interpretation of \"X1\" is that its effect on Y changes depending on the value of X2 (and vice versa).\n",
    "\n",
    "4. **Detecting Complex Relationships:**\n",
    "   - In real-world data, the relationship between the dependent and independent variables may not be purely linear. Interaction terms allow the model to capture these non-linear relationships, which might be crucial for making better predictions.\n",
    "   - **Example:**\n",
    "     - In a marketing analysis, the effect of advertising budget (X1) on sales (Y) might depend on the region (X2). In some regions, increasing the advertising budget could have a large impact, while in others it might have little to no effect.\n",
    "\n",
    "### How to Include Interaction Terms in the Model:\n",
    "1. **Mathematical Formulation:**\n",
    "   - Interaction terms are created by multiplying two or more predictor variables. For example, if you have two predictors, X1 and X2, the interaction term would be X1 * X2.\n",
    "   \n",
    "   The regression model with interaction terms might look like:\n",
    "     Y = β0 + β1 * X1 + β2 * X2 + β3 * (X1 * X2) + ε\n",
    "   Where:\n",
    "     - β0 is the intercept.\n",
    "     - β1, β2, and β3 are the coefficients of the predictors and the interaction term.\n",
    "     - ε is the error term.\n",
    "\n",
    "2. **Interpretation of Coefficients:**\n",
    "   - The coefficient of an interaction term (β3) represents how much the relationship between Y and one predictor changes for each unit change in the other predictor. If β3 is significant, the interaction between X1 and X2 has an important impact on the dependent variable.\n",
    "\n",
    "### When to Use Interaction Terms:\n",
    "- **When you suspect that the effect of one predictor on the dependent variable depends on another predictor.**\n",
    "- **When visualizing data shows that the relationship between predictors and the outcome varies at different levels of other predictors.**\n",
    "- **When conducting exploratory data analysis and uncovering complex relationships that cannot be captured by main effects alone.**\n",
    "\n",
    "### Caution:\n",
    "- **Overfitting:** Including too many interaction terms can lead to overfitting, where the model becomes too complex and starts to capture noise in the data rather than generalizable patterns.\n",
    "- **Multicollinearity:** Interaction terms can introduce multicollinearity, especially if the main effects are highly correlated with the interaction term.\n",
    "\n",
    "In summary, interaction terms in Multiple Linear Regression are essential for capturing the combined effect of predictors and improving model accuracy when predictors work together in influencing the dependent variable. However, it's important to carefully select and interpret these terms to avoid overfitting and multicollinearity.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c944dcbf-5cb3-46e4-9f30-92b1536f3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
    "'''\n",
    "The **intercept** in both Simple and Multiple Linear Regression represents the expected value of the dependent variable (Y) when all independent variables (X) are equal to zero. However, the interpretation of the intercept differs between Simple and Multiple Linear Regression due to the number of predictors involved in the model.\n",
    "\n",
    "### 1. **Intercept in Simple Linear Regression:**\n",
    "In Simple Linear Regression, there is only one predictor (X), and the equation for the model is:\n",
    "\n",
    "    Y = mX + c\n",
    "\n",
    "Where:\n",
    "- **m** is the slope (coefficient) of the predictor X.\n",
    "- **c** is the intercept, or the value of Y when X = 0.\n",
    "\n",
    "**Interpretation:**\n",
    "- In this case, the intercept **c** represents the expected value of Y when the independent variable X is zero.\n",
    "- **Example:**\n",
    "  - If you are predicting salary (Y) based on years of experience (X), the intercept represents the expected salary when someone has 0 years of experience (i.e., starting salary or base salary).\n",
    "\n",
    "**Limitations:**\n",
    "- The intercept in Simple Linear Regression is meaningful when X = 0 is within the range of observed values. If X = 0 is not a reasonable value (e.g., negative years of experience), the intercept may not be meaningful.\n",
    "\n",
    "### 2. **Intercept in Multiple Linear Regression:**\n",
    "In Multiple Linear Regression, there are two or more predictors, and the equation for the model is:\n",
    "\n",
    "    Y = β0 + β1X1 + β2X2 + ... + βnXn + ε\n",
    "\n",
    "Where:\n",
    "- **β0** is the intercept.\n",
    "- **β1, β2, ..., βn** are the coefficients for the predictors X1, X2, ..., Xn.\n",
    "- **ε** is the error term.\n",
    "\n",
    "**Interpretation:**\n",
    "- The intercept **β0** in Multiple Linear Regression represents the expected value of Y when all the independent variables (X1, X2, ..., Xn) are equal to zero.\n",
    "- **Example:**\n",
    "  - If you are predicting house price (Y) based on features like square footage (X1), number of bedrooms (X2), and neighborhood (X3), the intercept represents the expected house price when square footage, number of bedrooms, and neighborhood are all equal to zero. While this interpretation is mathematically valid, it may not be meaningful in practice because a house with zero square footage, zero bedrooms, and no neighborhood does not exist.\n",
    "\n",
    "**Limitations:**\n",
    "- In Multiple Linear Regression, the intercept can become less interpretable if the values of the independent variables (X1, X2, ..., Xn) being zero do not make sense in the context of the data. For example, a zero value for square footage or number of bedrooms is not a realistic scenario, so the intercept may not have a meaningful real-world interpretation.\n",
    "- The meaning of the intercept becomes more abstract as more variables are added to the model because it assumes that all predictors are zero simultaneously, which may be unrealistic.\n",
    "\n",
    "### Key Differences:\n",
    "1. **Number of Predictors:**\n",
    "   - In Simple Linear Regression, the intercept represents the value of Y when just one predictor is zero.\n",
    "   - In Multiple Linear Regression, the intercept represents the value of Y when all predictors are zero.\n",
    "\n",
    "2. **Real-World Interpretation:**\n",
    "   - The intercept in Simple Linear Regression often has a more straightforward interpretation, especially when the value of X = 0 is meaningful.\n",
    "   - The intercept in Multiple Linear Regression may have limited or no real-world meaning if the combination of zero values for all predictors does not represent a plausible scenario.\n",
    "\n",
    "### Conclusion:\n",
    "In both types of regression, the intercept provides a baseline value for Y when all predictors are zero. However, the interpretation becomes more abstract and potentially less meaningful as the number of predictors increases in Multiple Linear Regression. Careful consideration should be given to the context of the data when interpreting the intercept in either model.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1ed01-e2a6-4c10-9925-5480c85b8c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#16.What is the significance of the slope in regression analysis, and how does it affect predictions\n",
    "'''\n",
    "In regression analysis, the **slope** represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X). \n",
    "It quantifies the strength and direction of the relationship between the predictor and the outcome.\n",
    "\n",
    "### Significance of the Slope:\n",
    "1. **Magnitude of the Effect:**\n",
    "   - The slope (denoted as **m** in Simple Linear Regression or **β1, β2,...** in Multiple Linear Regression) indicates how much the dependent variable\n",
    "   (Y) will change when the independent variable (X) changes by one unit.\n",
    "   - **Example:**\n",
    "     - In Simple Linear Regression, if the slope is 2, it means that for every one-unit increase in X, Y will increase by 2 units. For instance, \n",
    "     if you're predicting salary (Y) based on years of experience (X), a slope of 2 means that for each additional year of experience, the salary \n",
    "     increases by 2 units (e.g., $2,000).\n",
    "\n",
    "2. **Direction of the Relationship:**\n",
    "   - The sign of the slope indicates the direction of the relationship:\n",
    "     - **Positive slope:** A positive value for the slope means that as X increases, Y also increases (a direct relationship).\n",
    "     - **Negative slope:** A negative value for the slope means that as X increases, Y decreases (an inverse relationship).\n",
    "   - **Example:**\n",
    "     - If the slope is -3 in a model predicting house prices (Y) based on the age of the house (X), this means that for every year older the house \n",
    "     becomes, the price decreases by 3 units (e.g., $3,000).\n",
    "\n",
    "3. **Rate of Change:**\n",
    "   - The slope also tells you the **rate of change** in the dependent variable for a given change in the independent variable. A larger absolute value \n",
    "   of the slope indicates a steeper rate of change, while a smaller absolute value indicates a gentler rate of change.\n",
    "\n",
    "### How the Slope Affects Predictions:\n",
    "1. **Prediction Formula:**\n",
    "   - In a Simple Linear Regression model, the equation is:\n",
    "     Y = mX + c\n",
    "     Where:\n",
    "     - Y is the predicted value.\n",
    "     - m is the slope, which indicates the rate of change.\n",
    "     - X is the value of the independent variable.\n",
    "     - c is the intercept, which is the value of Y when X = 0.\n",
    "   \n",
    "   - In Multiple Linear Regression, the equation expands to:\n",
    "     Y = β0 + β1X1 + β2X2 + ... + βnXn\n",
    "     Where:\n",
    "     - Y is the predicted value.\n",
    "     - β0 is the intercept.\n",
    "     - β1, β2,... are the slopes for the independent variables X1, X2, ..., Xn.\n",
    "\n",
    "2. **Effect on Predictions:**\n",
    "   - A positive slope increases the predicted value of Y as the independent variable (X) increases.\n",
    "   - A negative slope decreases the predicted value of Y as the independent variable (X) increases.\n",
    "   - The steeper the slope, the more sensitive the predicted value of Y will be to changes in X.\n",
    "\n",
    "   **Example:**\n",
    "   - In a marketing analysis predicting sales (Y) based on advertising budget (X), if the slope is 100, it means that for every additional dollar spent\n",
    "   on advertising, sales increase by 100 units. This is a strong predictor with a significant impact on the outcome.\n",
    "\n",
    "3. **Significance Testing:**\n",
    "   - The significance of the slope is assessed through hypothesis testing (usually a t-test), which helps determine whether the relationship between X \n",
    "   and Y is statistically significant. A significant slope means that X has a meaningful effect on Y, while a non-significant slope suggests that X does not contribute to explaining Y.\n",
    "\n",
    "   **Example:**\n",
    "   - A t-test on the slope might show that the slope is significantly different from zero, indicating that changes in X are significantly related to\n",
    "   changes in Y.\n",
    "\n",
    "### Conclusion:\n",
    "The slope in regression analysis is crucial because it quantifies the relationship between the independent and dependent variables, guiding predictions.\n",
    "It provides both the magnitude and direction of the change in Y for a given change in X, affecting how the model predicts future outcomes. \n",
    "The slope’s significance is also critical in assessing the reliability of the relationship between predictors and the dependent variable.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28489f3-1a24-4893-8a14-5a3b8d5112fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17.How does the intercept in a regression model provide context for the relationship between variables\n",
    "'''\n",
    "\n",
    "\n",
    "In regression analysis, the **intercept** (denoted as **c** in Simple Linear Regression or **β0** in Multiple Linear Regression) is a constant term that represents the expected value of the dependent variable (Y) when all independent variables (X) are equal to zero. The intercept provides a starting point or baseline for the regression model, helping to contextualize the relationship between the independent and dependent variables.\n",
    "\n",
    "### How the Intercept Provides Context:\n",
    "1. **Starting Point of the Model:**\n",
    "   - The intercept gives the predicted value of Y when the independent variables are all equal to zero. This can be interpreted as the baseline value of Y in the absence of any influence from the predictors.\n",
    "   - **Example (Simple Linear Regression):**\n",
    "     - In a model predicting salary (Y) based on years of experience (X), the intercept would represent the expected salary when someone has 0 years of experience. In this case, it could be the starting salary or base salary that an individual would receive regardless of their experience.\n",
    "   \n",
    "   **Formula:**\n",
    "     - Y = mX + c\n",
    "     - When X = 0, Y = c, which represents the intercept value.\n",
    "\n",
    "2. **Contextual Interpretation of the Relationship:**\n",
    "   - The intercept provides **context** for how the dependent variable (Y) behaves when the independent variable (X) is at a value of zero.\n",
    "   - The meaning of the intercept can vary based on the variables in the model and the context of the data. If the intercept represents an unrealistic or nonsensical scenario (like zero years of experience or zero square footage of a house), it might not be meaningful in a practical sense but still remains mathematically valid.\n",
    "   - **Example (Multiple Linear Regression):**\n",
    "     - In a model predicting house price (Y) based on square footage (X1), number of bedrooms (X2), and location (X3), the intercept (β0) represents the price of a house when all predictors (square footage, bedrooms, location) are zero. While this might not correspond to a real-world scenario (a house with zero square footage, no bedrooms, and no location), it serves as the baseline starting point for the model.\n",
    "\n",
    "3. **Understanding the Relationship Between Variables:**\n",
    "   - The intercept is crucial for understanding how changes in the independent variables (X) influence the dependent variable (Y). It serves as the reference point from which the effects of the independent variables are measured.\n",
    "   - **Example (Simple Linear Regression):**\n",
    "     - If you're modeling the relationship between advertising budget (X) and sales (Y), the intercept tells you the level of sales when no money is spent on advertising (X = 0). If the intercept is 500 units, it means that, even without advertising, you would expect 500 units in sales.\n",
    "   - **Example (Multiple Linear Regression):**\n",
    "     - In a model with multiple predictors (e.g., price of a car as a function of brand, age, and mileage), the intercept would provide the expected price of the car when the brand is zero (or the reference category), the car's age is zero, and the mileage is zero. While such a scenario is unrealistic, the intercept provides a contextual starting point for understanding how each factor influences price.\n",
    "\n",
    "4. **Baseline for Prediction:**\n",
    "   - The intercept provides the **baseline prediction** for Y, serving as the point from which the contributions of each independent variable (through their respective coefficients) are added or subtracted. \n",
    "   - **Example:**\n",
    "     - If you're predicting a student’s GPA based on hours studied and class participation, the intercept would represent the GPA when both hours studied and class participation are zero. This gives a baseline GPA to work from, which is then adjusted based on the input values for hours studied and participation.\n",
    "\n",
    "5. **Impact on Model Interpretation:**\n",
    "   - When interpreting regression models, especially with multiple predictors, the intercept helps to understand how each predictor modifies the outcome relative to this baseline.\n",
    "   - **Example (Multiple Linear Regression):**\n",
    "     - In a model predicting health outcomes (Y) based on exercise hours (X1), diet quality (X2), and sleep hours (X3), the intercept represents the health outcome when all these factors are at their baseline (e.g., zero exercise, poor diet, and no sleep). While this might not be a realistic situation, it provides a reference for understanding how the combination of these factors affects health outcomes.\n",
    "\n",
    "### Limitations of the Intercept:\n",
    "1. **Unrealistic Contexts:**\n",
    "   - In some cases, the intercept may represent a scenario that does not make sense in the real world, such as negative values for physical quantities (e.g., negative price, negative square footage). In these cases, while the intercept is mathematically valid, its real-world interpretation may be limited or nonsensical.\n",
    "   \n",
    "2. **Non-Meaningful for Certain Data:**\n",
    "   - When the independent variables cannot take the value of zero in practice, the intercept loses its direct interpretability. However, it still plays a role in determining the overall fit and predictions of the model.\n",
    "\n",
    "### Conclusion:\n",
    "The intercept in regression models provides essential context for the relationship between the independent and dependent variables. It serves as the baseline value for the dependent variable when all independent variables are zero and helps to interpret the overall behavior of the model. While its real-world meaning can sometimes be limited or unrealistic, it is crucial for understanding how the independent variables influence the dependent variable and for making predictions.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab05d4d1-f782-4ae2-a225-6cf0364ad845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18 What are the limitations of using R² as a sole measure of model performance.\n",
    "'''\n",
    "B4- What are the limitations of using R² as a sole measure of model performance?\n",
    "\n",
    "The **coefficient of determination** (R²) is a commonly used metric for assessing the goodness of fit in regression models. It quantifies the proportion of variance in the dependent variable that is explained by the independent variables. Although R² is useful, it has several limitations when used as the sole measure of model performance. \n",
    "\n",
    "### Limitations of R²:\n",
    "\n",
    "1. **R² Always Increases with More Predictors:**\n",
    "   - One of the primary limitations of R² is that it **increases** as more independent variables (predictors) are added to the model, even if those variables are not truly meaningful or predictive. This can lead to **overfitting**, where the model seems to explain a high proportion of the variance in the data, but in reality, it is too complex and captures noise.\n",
    "   - **Example:**\n",
    "     - Adding irrelevant predictors to a model can increase R², making it appear that the model fits the data better when, in fact, it is just fitting noise.\n",
    "\n",
    "2. **Does Not Indicate Causality:**\n",
    "   - R² measures the strength of the linear relationship between the predictors and the dependent variable, but it does not imply causality. A high R² does not mean that the independent variables are causing changes in the dependent variable.\n",
    "   - **Example:**\n",
    "     - A high R² between the number of ice creams sold and the number of drownings in summer months may suggest a strong relationship, but it doesn't imply that selling ice cream causes drownings. The relationship could be due to a third factor, such as warm weather.\n",
    "\n",
    "3. **Insensitive to Non-Linear Relationships:**\n",
    "   - R² measures the goodness of fit for linear models. If the true relationship between the independent and dependent variables is non-linear, R² may not capture the model's performance adequately, even if the model fits the data well.\n",
    "   - **Example:**\n",
    "     - If the relationship between predictors and the outcome is quadratic or exponential, R² might not reflect how well the model captures the data.\n",
    "\n",
    "4. **Does Not Account for Model Complexity:**\n",
    "   - R² does not take into account the complexity of the model (e.g., the number of predictors). It is possible to have a high R² with a complex model that overfits the data, making it poor at generalizing to new data.\n",
    "   - **Example:**\n",
    "     - A model with too many predictors might produce a high R² on training data but fail to generalize well to test data due to overfitting.\n",
    "\n",
    "5. **Does Not Provide Information on Model Accuracy:**\n",
    "   - R² does not directly measure how accurate the model's predictions are. It tells you how well the model fits the data in terms of variance explained but provides no insight into prediction error or bias.\n",
    "   - **Example:**\n",
    "     - A model with a high R² could still have large prediction errors if the predictions are biased or inaccurate in certain regions of the data.\n",
    "\n",
    "6. **Sensitive to Outliers:**\n",
    "   - R² can be heavily influenced by outliers. A few extreme data points can artificially inflate or deflate the R² value, giving a misleading impression of the model’s fit.\n",
    "   - **Example:**\n",
    "     - If a model includes outliers that do not represent the typical data distribution, the R² value may be misleadingly high or low.\n",
    "\n",
    "7. **No Information on the Direction of Relationship:**\n",
    "   - R² measures how well the model fits the data but does not tell you whether the relationship between variables is positive or negative. You need to inspect the coefficients of the regression model for that information.\n",
    "   - **Example:**\n",
    "     - While R² tells you how much variance is explained, it does not give any indication of whether increasing the value of an independent variable leads to an increase or decrease in the dependent variable.\n",
    "\n",
    "### Alternatives and Complementary Metrics:\n",
    "To mitigate the limitations of using R² as the sole measure of model performance, you can use complementary metrics such as:\n",
    "- **Adjusted R²:** Adjusts R² for the number of predictors in the model, penalizing overfitting.\n",
    "- **Root Mean Squared Error (RMSE):** Measures the average magnitude of prediction errors, giving insight into how well the model predicts individual data points.\n",
    "- **Mean Absolute Error (MAE):** Measures the average absolute difference between predicted and actual values.\n",
    "- **Cross-validation:** Evaluates the model's performance on different subsets of the data to assess its generalization ability.\n",
    "\n",
    "### Conclusion:\n",
    "While R² is a valuable metric for evaluating model fit, it has several limitations when used in isolation. It can be misleading, especially in the presence of overfitting, non-linear relationships, outliers, and model complexity. It is crucial to use additional evaluation metrics, such as adjusted R², RMSE, and cross-validation, to obtain a more comprehensive understanding of model performance and ensure the model generalizes well to unseen data.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95914ae5-cd45-4d7d-9d00-a3c195c625a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#19.How would you interpret a large standard error for a regression coefficient.\n",
    "'''\n",
    "In regression analysis, the **standard error** of a regression coefficient is a measure of the variability or uncertainty associated with the\n",
    "estimated coefficient. A large standard error for a regression coefficient suggests that the estimated value of the coefficient is highly uncertain \n",
    "or imprecise.\n",
    "\n",
    "### Interpretation of a Large Standard Error:\n",
    "1. **Low Precision of Estimate:**\n",
    "   - A large standard error indicates that the estimate of the regression coefficient is less precise. The larger the standard error, the wider the \n",
    "   confidence interval around the coefficient estimate, implying more uncertainty about the true value of the coefficient.\n",
    "   - **Example:**\n",
    "     - If the coefficient estimate for a predictor is 5 with a standard error of 10, the true value of the coefficient could be anywhere between -5 \n",
    "     and 15 with a certain level of confidence. This wide range suggests a lack of precision.\n",
    "\n",
    "2. **Possible Lack of Significance:**\n",
    "   - The standard error is used to calculate the **t-statistic** (t = coefficient / standard error), which is then used to assess whether the\n",
    "   coefficient is statistically significant. A large standard error results in a small t-statistic, making it more likely that the coefficient is\n",
    "   not statistically significant.\n",
    "   - **Example:**\n",
    "     - If the t-statistic is very small (due to a large standard error), the null hypothesis that the coefficient is zero (i.e., no effect) is \n",
    "     harder to reject, suggesting that the predictor may not be a meaningful contributor to the model.\n",
    "\n",
    "3. **Multicollinearity:**\n",
    "   - A large standard error may also indicate issues with **multicollinearity**, where two or more independent variables are highly correlated \n",
    "   with each other. Multicollinearity makes it difficult to isolate the individual effects of each predictor on the dependent variable, leading \n",
    "   to inflated standard errors.\n",
    "   - **Example:**\n",
    "     - In a model with both years of education and income as predictors, if these two variables are highly correlated, the standard errors for \n",
    "     the coefficients may increase, making it harder to determine the unique contribution of each predictor.\n",
    "\n",
    "4. **Small Sample Size:**\n",
    "   - A large standard error can be the result of a **small sample size**. With fewer data points, there is more variability in the estimates, \n",
    "   leading to higher standard errors.\n",
    "   - **Example:**\n",
    "     - In a study with only 10 data points, the standard errors of the regression coefficients will likely be large, even if the relationship\n",
    "     between the predictor and the dependent variable is strong.\n",
    "\n",
    "5. **Poor Model Fit or Inadequate Data:**\n",
    "   - A large standard error might indicate that the model does not fit the data well or that there is a poor relationship between the independent \n",
    "   variables and the dependent variable. This could happen due to omitted variables, measurement errors, or other data issues.\n",
    "   - **Example:**\n",
    "     - If the regression model doesn't account for important factors, the standard errors of the coefficients for the included predictors could be \n",
    "     large, indicating that the model is not well-specified.\n",
    "\n",
    "6. **Potential for Overfitting or Underfitting:**\n",
    "   - In some cases, a model that is overfitting or underfitting the data can lead to large standard errors. Overfitting leads to high variance in \n",
    "   the coefficient estimates, while underfitting may not capture the full relationship between predictors and the outcome, both of which can result\n",
    "   in large standard errors.\n",
    "   - **Example:**\n",
    "     - A model with too many predictors relative to the sample size may overfit, leading to large standard errors for the coefficients.\n",
    "\n",
    "### Conclusion:\n",
    "A large standard error for a regression coefficient signals a high level of uncertainty about the true value of the coefficient. This could be due to \n",
    "various factors, including poor model fit, multicollinearity, small sample size, or imprecise data. It is important to interpret large standard errors\n",
    "with caution, as they suggest that the model's estimates are not reliable. In such cases, it is often beneficial to reassess the model, consider\n",
    "removing highly correlated predictors, or increase the sample size to improve the precision of the estimates.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39342899-8312-40d5-a132-e9b4b1d20915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20.How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
    "'''\n",
    "\n",
    "\n",
    "**Heteroscedasticity** refers to the condition in regression analysis where the variability of the residuals (errors) is not constant across all \n",
    "levels of the independent variable(s). In other words, the spread or dispersion of the residuals changes as the value of the predictor(s) changes. \n",
    "It can significantly affect the validity of the regression results, including biasing statistical tests, inflating standard errors, and making the \n",
    "model's predictions less reliable.\n",
    "\n",
    "### Identifying Heteroscedasticity in Residual Plots:\n",
    "A **residual plot** is a graphical tool used to assess whether the residuals exhibit any patterns or non-constant variance. It is typically a scatter\n",
    "plot of the residuals (errors) on the y-axis versus the predicted values (or one of the independent variables) on the x-axis. Here's how you can\n",
    "identify heteroscedasticity:\n",
    "\n",
    "1. **Non-constant Spread:**\n",
    "   - In the presence of heteroscedasticity, the spread of residuals increases or decreases systematically as the predicted values change. \n",
    "   - **Common Patterns of Heteroscedasticity:**\n",
    "     - **Funnel Shape:** The residuals are tightly clustered around the zero line for low predicted values but become more spread out as the \n",
    "     predicted values increase. This indicates that the variance of the errors increases with the value of the predicted variable.\n",
    "     - **Inverted Funnel:** The residuals spread out more as predicted values decrease, suggesting decreasing variance with increasing predicted values.\n",
    "     \n",
    "     - **Other Patterns:** Sometimes, residuals can fan out or contract in a nonlinear manner, showing that variance is changing unpredictably as the \n",
    "     independent variable(s) change.\n",
    "\n",
    "2. **Random Scatter Without Trend (Homogeneous Variance):**\n",
    "   - In contrast, if the residual plot shows a random scatter of residuals with no discernible pattern and constant spread across all levels of the \n",
    "   predicted values or the independent variable(s), this indicates **homoscedasticity** (constant variance).\n",
    "\n",
    "3. **Example:**\n",
    "   - If you're modeling house prices as a function of square footage, and the residuals increase in spread as the house size increases, this indicates\n",
    "   heteroscedasticity.\n",
    "\n",
    "### Why Is It Important to Address Heteroscedasticity?\n",
    "\n",
    "1. **Violation of Assumptions:**\n",
    "   - Heteroscedasticity violates one of the key assumptions of ordinary least squares (OLS) regression: that the variance of the errors is constant\n",
    "   across all levels of the independent variable(s) (homoscedasticity). When this assumption is violated, the model's statistical tests, confidence\n",
    "   intervals, and predictions become unreliable.\n",
    "\n",
    "2. **Inflated Standard Errors:**\n",
    "   - If heteroscedasticity is present, the standard errors of the regression coefficients may be biased and inconsistent, leading to incorrect \n",
    "   hypothesis tests and confidence intervals. This can result in:\n",
    "     - **Type I errors** (incorrectly rejecting the null hypothesis when it is true).\n",
    "     - **Type II errors** (failing to reject the null hypothesis when it is false).\n",
    "\n",
    "3. **Inefficient Estimates:**\n",
    "   - While OLS estimates of the regression coefficients remain unbiased in the presence of heteroscedasticity, they are no longer efficient \n",
    "   (i.e., they do not have the smallest possible variance). This means that the estimates may not be the most reliable or precise.\n",
    "\n",
    "4. **Incorrect Predictions:**\n",
    "   - Heteroscedasticity can affect the accuracy of the predictions made by the model. The model might give large errors for certain values of the\n",
    "   independent variables, leading to inaccurate or misleading predictions.\n",
    "\n",
    "### How to Address Heteroscedasticity:\n",
    "1. **Transform the Dependent Variable:**\n",
    "   - Applying a transformation to the dependent variable, such as a log or square root transformation, can often stabilize the variance of the \n",
    "   residuals and reduce heteroscedasticity.\n",
    "   - **Example:**\n",
    "     - If the dependent variable is income and the residuals show increasing spread as income increases, applying a logarithmic transformation to \n",
    "     the income variable may help stabilize the variance.\n",
    "\n",
    "2. **Weighted Least Squares (WLS) Regression:**\n",
    "   - WLS is an alternative to OLS that gives different weights to different data points based on the variance of their residuals. It helps correct \n",
    "   for heteroscedasticity by giving more weight to observations with smaller error variance and less weight to observations with larger error variance.\n",
    "\n",
    "3. **Robust Standard Errors:**\n",
    "   - If transforming the data or using WLS is not suitable, one can use **robust standard errors** (also called heteroscedasticity-consistent standard\n",
    "   errors), which adjust the calculation of standard errors to account for heteroscedasticity, thereby providing more reliable hypothesis tests \n",
    "   and confidence intervals.\n",
    "\n",
    "4. **Model Specification:**\n",
    "   - Heteroscedasticity may arise due to a misspecified model, such as failing to include an important predictor or using the wrong functional form\n",
    "   (e.g., linear when the relationship is nonlinear). Ensuring that the model is correctly specified can sometimes mitigate heteroscedasticity.\n",
    "\n",
    "### Conclusion:\n",
    "Heteroscedasticity is an important issue in regression analysis because it undermines the reliability of statistical tests and model predictions. \n",
    "Identifying it through residual plots is crucial for diagnosing model problems, and addressing it is necessary for obtaining valid regression results.\n",
    "Methods such as transforming the dependent variable, using weighted least squares regression, or applying robust standard errors can help to correct for\n",
    "heteroscedasticity and improve the performance and interpretability of the regression model.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6116bfc4-78f1-4320-8bc8-ce4f5e71e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#21.What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R².\n",
    "'''\n",
    "In Multiple Linear Regression, the **R² (coefficient of determination)** and **adjusted R²** are both used to assess the goodness of fit of a model, but they provide different insights. While R² indicates the proportion of variance in the dependent variable explained by the independent variables, **adjusted R²** accounts for the number of predictors in the model and penalizes for adding irrelevant variables. \n",
    "\n",
    "### High R² but Low Adjusted R²:\n",
    "When a Multiple Linear Regression model has a high R² but a low adjusted R², it typically indicates that the model may be overfitting the data. This happens when adding more predictors to the model increases R², but those predictors do not contribute significantly to explaining the variance in the dependent variable. Here's what this situation means:\n",
    "\n",
    "1. **Overfitting:**\n",
    "   - A high R² means the model explains a large proportion of the variance in the dependent variable. However, when adjusted R² is low, it suggests that the model may have too many predictors or that some of the predictors do not genuinely improve the model’s explanatory power.\n",
    "   - **Overfitting** occurs when the model becomes too complex, capturing not only the true relationships in the data but also the random noise or fluctuations. This can result in a model that performs well on the training data but poorly on unseen data (test data).\n",
    "   \n",
    "2. **Irrelevant or Redundant Predictors:**\n",
    "   - If the model includes unnecessary or highly correlated predictors, R² may increase simply because more variables are included, but adjusted R² will be low because it penalizes the inclusion of such variables.\n",
    "   - In this case, the model may become more complex without actually improving its ability to explain the variance in the dependent variable.\n",
    "\n",
    "3. **R² Can Be Misleading:**\n",
    "   - Since R² always increases or stays the same when new predictors are added to the model (even if those predictors are irrelevant), it is not a reliable indicator of the model’s quality when comparing models with different numbers of predictors. A high R² with low adjusted R² suggests that the increase in explanatory power is due to the addition of variables that do not add meaningful predictive value.\n",
    "\n",
    "4. **Adjusted R² Penalizes for Model Complexity:**\n",
    "   - Adjusted R² adjusts the R² value by taking into account the number of predictors in the model. When the model has too many predictors relative to the sample size or includes irrelevant variables, adjusted R² will decrease, highlighting that the model's fit is not as good as it might seem from R² alone.\n",
    "   - A low adjusted R² in this case serves as a warning sign that the model may not generalize well to new data, even though R² suggests otherwise.\n",
    "\n",
    "### Example:\n",
    "Imagine you're building a model to predict house prices based on several predictors such as square footage, number of bedrooms, location, and age of the house. If you add irrelevant variables, like the color of the house or the owner's name, R² might increase simply because you have more variables in the model, even though these new variables don't actually explain the price variation. Adjusted R² will lower to reflect the lack of genuine improvement in model performance.\n",
    "\n",
    "### Why It's Important:\n",
    "1. **Model Parsimony:**\n",
    "   - The goal in regression modeling is to have a model that explains as much variance in the dependent variable as possible while using the fewest predictors necessary (a principle known as **Occam's Razor**). A high R² but low adjusted R² suggests that the model might be too complex and includes redundant or unnecessary predictors, which might reduce its ability to generalize to new data.\n",
    "\n",
    "2. **Generalization to New Data:**\n",
    "   - A model with a high R² but low adjusted R² is likely overfitting and may not perform well when applied to new, unseen data. By focusing on adjusted R², you get a better sense of how well the model is likely to perform on future data, helping you avoid building overly complex models that don’t provide real predictive value.\n",
    "\n",
    "### Conclusion:\n",
    "A high R² but low adjusted R² suggests that the Multiple Linear Regression model may be overfitting and that it includes irrelevant or unnecessary predictors. While R² can be a useful measure of fit, adjusted R² provides a more accurate representation of model performance, particularly when comparing models with different numbers of predictors. It is essential to rely on adjusted R² to assess whether the model has sufficient explanatory power while avoiding unnecessary complexity.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc43fe8-cbe1-45b9-9225-660c452d72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#22.Why is it important to scale variables in Multiple Linear Regression\n",
    "'''\n",
    "\n",
    "Scaling variables is a crucial step in Multiple Linear Regression, especially when the independent variables (predictors) have different units or vastly\n",
    "different ranges. Scaling ensures that all variables contribute equally to the model and prevents some variables from dominating others due to their\n",
    "larger magnitude or variance. Here's why it's important:\n",
    "\n",
    "### 1. **Ensures Equal Contribution of Variables:**\n",
    "   - In a Multiple Linear Regression model, the coefficients represent the change in the dependent variable (Y) for a one-unit change in the independent\n",
    "   variables (X). If the variables have different scales, variables with larger numerical ranges will disproportionately influence the model's\n",
    "   coefficients, making it harder to assess their individual importance.\n",
    "   - **Example:** \n",
    "     - Consider a model with one predictor in thousands of dollars (e.g., income) and another in single digits (e.g., age). The income variable will \n",
    "     have much larger numerical values than age, and this could lead to the income variable having a much greater effect on the model’s outcome, \n",
    "     regardless of its actual importance.\n",
    "\n",
    "### 2. **Improves Interpretability of Coefficients:**\n",
    "   - When variables are on different scales, the interpretation of the regression coefficients can be misleading. Scaling standardizes all variables to \n",
    "   a common range (typically a mean of 0 and a standard deviation of 1), which makes the coefficients more comparable.\n",
    "   - **Example:**\n",
    "     - If one variable is measured in years and another in square footage, scaling both will allow you to better compare the impact of changes in each\n",
    "     predictor on the dependent variable, as the coefficients will be on a similar scale.\n",
    "\n",
    "### 3. **Prevents Numerical Instability:**\n",
    "   - Some regression algorithms, particularly those that use gradient descent optimization (like regularized regression models), are sensitive to the \n",
    "   scale of the variables. If one variable has much larger values than another, it could cause issues with the optimization process, leading to **slow\n",
    "   convergence** or even **non-convergence** of the model.\n",
    "   - **Example:** \n",
    "     - In algorithms like **Ridge Regression** or **Lasso Regression**, which apply regularization penalties, the scale of the variables affects how the\n",
    "     penalty is applied. Without scaling, variables with larger values might receive a smaller regularization penalty, resulting in a biased model.\n",
    "\n",
    "### 4. **Improves Model Performance (Regularization Methods):**\n",
    "   - Scaling is especially important when using regularization techniques like **Lasso (L1)** or **Ridge (L2)** regression, where penalties are applied \n",
    "   to the coefficients to prevent overfitting. If variables are not scaled, the regularization might disproportionately penalize certain predictors, \n",
    "   leading to biased results.\n",
    "   - **Example:**\n",
    "     - Without scaling, Ridge Regression might shrink a variable with a large range less than a variable with a small range, even if both variables are\n",
    "     equally important for the model.\n",
    "\n",
    "### 5. **Prevents Skewing of Distance Metrics (for Some Models):**\n",
    "   - In some machine learning algorithms that use distance metrics (e.g., k-nearest neighbors or support vector machines), scaling ensures that each \n",
    "   variable contributes equally to the distance calculation. Variables with larger scales would dominate the distance measure if not scaled.\n",
    "   - **Example:**\n",
    "     - In k-nearest neighbors regression, features like income or population size could skew the distance calculation unless all features are scaled to\n",
    "     have similar magnitudes.\n",
    "\n",
    "### 6. **Facilitates Better Convergence in Optimization:**\n",
    "   - When performing **gradient descent** (as in fitting some types of regression models), scaling the features ensures that the algorithm converges\n",
    "   faster. Features with different magnitudes or units can cause the gradient descent algorithm to take longer to converge because the learning rate \n",
    "   may have to be adjusted to account for the differences in scales.\n",
    "   \n",
    "### Methods for Scaling:\n",
    "1. **Standardization (Z-score Scaling):**\n",
    "   - Subtract the mean and divide by the standard deviation for each feature. This ensures that each feature has a mean of 0 and a standard deviation \n",
    "   of 1.\n",
    "   - Formula: `X_scaled = (X - mean) / standard deviation`\n",
    "\n",
    "2. **Min-Max Scaling:**\n",
    "   - Rescales the data to a fixed range, usually [0, 1], by subtracting the minimum value and dividing by the range (max - min).\n",
    "   - Formula: `X_scaled = (X - min) / (max - min)`\n",
    "\n",
    "3. **Robust Scaling:**\n",
    "   - Uses the median and interquartile range (IQR) to scale the data, which makes it more robust to outliers.\n",
    "\n",
    "### Conclusion:\n",
    "Scaling variables in Multiple Linear Regression is important to ensure that each predictor is treated equally and contributes proportionately to the\n",
    "model. It improves model performance by ensuring numerical stability, facilitates better optimization, and enhances the interpretability of the \n",
    "coefficients. Scaling is particularly crucial when using regularized regression techniques or models that depend on distance metrics. By scaling your \n",
    "variables, you can ensure that the regression model is both accurate and reliable.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9a569-8f0a-4785-9729-85d1f3797e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#23.What is polynomial regression\n",
    "'''\n",
    "\n",
    "\n",
    "Polynomial Regression is an extension of Simple Linear Regression that allows for the modeling of non-linear relationships between the dependent\n",
    "and independent variables. Instead of fitting a straight line (as in linear regression), polynomial regression fits a polynomial (curved) relationship,\n",
    "which can better capture complex patterns in the data.\n",
    "\n",
    "### Key Concepts of Polynomial Regression:\n",
    "\n",
    "1. **Polynomial Equation:**\n",
    "   - A polynomial regression model uses a polynomial function to model the relationship between the independent variable(s) and the dependent variable.\n",
    "   - The general form of the polynomial regression equation is:\n",
    "     - **Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βnXⁿ + ε**\n",
    "     - Here, Y is the dependent variable, X is the independent variable, β₀ is the intercept, β₁, β₂, ..., βn are the coefficients, and X², X³, ..., Xⁿ\n",
    "     represent the polynomial terms.\n",
    "   \n",
    "2. **Non-linear Relationship:**\n",
    "   - Unlike simple linear regression that assumes a linear relationship between X and Y, polynomial regression models non-linear relationships. \n",
    "   It uses higher-degree polynomial terms (X², X³, ...) to capture the curvature in the data.\n",
    "   - **Example:**\n",
    "     - In a dataset where the relationship between variables is not linear (e.g., a U-shaped or inverted U-shaped curve), polynomial regression can \n",
    "     provide a better fit to the data than linear regression.\n",
    "\n",
    "3. **Degrees of the Polynomial:**\n",
    "   - The degree of the polynomial (n in Xⁿ) determines the complexity of the model:\n",
    "     - **Degree 1** is simply linear regression (a straight line).\n",
    "     - **Degree 2** is quadratic regression (a parabola).\n",
    "     - **Degree 3** is cubic regression (a curve with one inflection point), and so on.\n",
    "   - Higher-degree polynomials can capture more complex relationships, but they also run the risk of **overfitting** the data, especially with a small \n",
    "   dataset.\n",
    "   \n",
    "4. **Fitting the Polynomial:**\n",
    "   - Polynomial regression fits the polynomial equation to the data using **least squares estimation**, just like in linear regression. The goal is to\n",
    "   find the values of the coefficients (β₀, β₁, ..., βn) that minimize the sum of squared differences between the predicted values and the actual values.\n",
    "   \n",
    "5. **Application:**\n",
    "   - Polynomial regression is commonly used when the relationship between the independent variable and the dependent variable is not adequately captured \n",
    "   by a straight line, but rather by a curve.\n",
    "   - **Example:** \n",
    "     - Modeling the growth of a population, where the relationship between time and population might be exponential or quadratic, or modeling the trajectory of a projectile.\n",
    "\n",
    "### Benefits of Polynomial Regression:\n",
    "- **Captures Non-Linear Relationships:**\n",
    "  - It can model more complex data relationships that linear regression cannot.\n",
    "- **Flexibility:**\n",
    "  - By increasing the degree of the polynomial, the model can fit various types of curves, providing flexibility to the model.\n",
    "- **Better Fit:**\n",
    "  - In cases where the data has a clearly non-linear pattern, polynomial regression can provide a better fit than linear regression.\n",
    "\n",
    "### Challenges and Considerations:\n",
    "1. **Overfitting:**\n",
    "   - Higher-degree polynomials can lead to **overfitting**, where the model becomes too complex and fits not only the true data but also the noise. \n",
    "   This results in a model that performs poorly on new, unseen data.\n",
    "   - **Example:**\n",
    "     - A degree-10 polynomial might perfectly fit the training data but fail to generalize to test data.\n",
    "   \n",
    "2. **Increased Complexity:**\n",
    "   - As the degree of the polynomial increases, the model becomes more complex and harder to interpret. It can also require more data to avoid \n",
    "   overfitting.\n",
    "   \n",
    "3. **Extrapolation Issues:**\n",
    "   - Polynomial regression models may perform poorly when extrapolating beyond the range of the training data because the polynomial curve may grow \n",
    "   unreasonably in regions where no data is available.\n",
    "   \n",
    "4. **Multicollinearity:**\n",
    "   - Including high-degree polynomial terms can lead to multicollinearity (high correlation between predictors), which can cause issues with the \n",
    "   estimation of coefficients and make the model unstable.\n",
    "\n",
    "### Conclusion:\n",
    "Polynomial Regression is a useful technique for modeling non-linear relationships in data. By using polynomial terms, it can capture curves and complex\n",
    "patterns that linear regression cannot. However, it is important to carefully choose the degree of the polynomial to avoid overfitting and ensure the \n",
    "model generalizes well to new data. Like all models, polynomial regression should be used with caution, particularly when interpreting higher-degree \n",
    "terms, as they can increase model complexity and reduce interpretability.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935aed37-9b76-4867-a1b9-e97822fb99b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#24.How does polynomial regression differ from linear regression\n",
    "'''\n",
    "Polynomial Regression and Linear Regression are both techniques used to model the relationship between a dependent variable (Y) and one or more independent variables (X), but they differ in the type of relationship they model and how they fit the data.\n",
    "\n",
    "### 1. **Relationship Between Variables:**\n",
    "   - **Linear Regression:** \n",
    "     - Linear regression assumes a **linear** relationship between the dependent and independent variables. In other words, the relationship is represented by a straight line.\n",
    "     - The model is of the form:\n",
    "       - **Y = β₀ + β₁X + ε**\n",
    "     - Here, Y is the dependent variable, X is the independent variable, β₀ is the intercept, β₁ is the slope (coefficient), and ε is the error term.\n",
    "\n",
    "   - **Polynomial Regression:** \n",
    "     - Polynomial regression, on the other hand, models **non-linear** relationships between the dependent and independent variables. It introduces polynomial terms of the independent variable (e.g., X², X³) to capture curvature in the data.\n",
    "     - The model is of the form:\n",
    "       - **Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βnXⁿ + ε**\n",
    "     - Here, the polynomial terms (X², X³, ...) allow the model to fit more complex relationships, such as parabolic or cubic curves.\n",
    "\n",
    "### 2. **Model Type (Linear vs. Non-Linear):**\n",
    "   - **Linear Regression:**\n",
    "     - Fits a **straight line** through the data.\n",
    "     - It can only capture **linear** relationships, meaning the dependent variable changes at a constant rate with respect to the independent variable.\n",
    "   \n",
    "   - **Polynomial Regression:**\n",
    "     - Fits a **curved line** through the data.\n",
    "     - It can capture **non-linear** relationships, where the dependent variable changes at a varying rate depending on the value of the independent variable(s).\n",
    "\n",
    "### 3. **Equation Complexity:**\n",
    "   - **Linear Regression:**\n",
    "     - The equation is simple and contains only linear terms of the predictors. For a single predictor, the equation is simply **Y = β₀ + β₁X + ε**.\n",
    "   \n",
    "   - **Polynomial Regression:**\n",
    "     - The equation is more complex, as it includes polynomial terms (X², X³, ...) for the predictors. The degree of the polynomial (n in Xⁿ) determines the model's complexity.\n",
    "\n",
    "### 4. **Flexibility in Modeling Data:**\n",
    "   - **Linear Regression:**\n",
    "     - Linear regression is limited in its ability to model complex patterns. It can only fit straight lines to the data and is suitable for data that follows a linear trend.\n",
    "     - **Example:** A simple relationship like predicting height based on age, where the relationship is roughly linear.\n",
    "   \n",
    "   - **Polynomial Regression:**\n",
    "     - Polynomial regression offers greater flexibility, as it can fit curves of varying complexity (quadratic, cubic, etc.) to the data.\n",
    "     - **Example:** A relationship like the growth of a population, where the rate of change may accelerate or decelerate over time.\n",
    "\n",
    "### 5. **Risk of Overfitting:**\n",
    "   - **Linear Regression:**\n",
    "     - Linear regression is less prone to overfitting when compared to polynomial regression, especially when the relationship is truly linear. However, overfitting can still occur if the model includes irrelevant predictors.\n",
    "   \n",
    "   - **Polynomial Regression:**\n",
    "     - Polynomial regression is more prone to **overfitting**, especially when using higher-degree polynomials. As the degree of the polynomial increases, the model becomes more flexible and can fit the noise in the data, leading to overfitting. This reduces the model’s ability to generalize to new, unseen data.\n",
    "     - **Example:** Using a high-degree polynomial to fit a small dataset can result in a curve that perfectly fits the training data but performs poorly on test data.\n",
    "\n",
    "### 6. **Interpretability:**\n",
    "   - **Linear Regression:**\n",
    "     - Linear regression is easier to interpret because it fits a straight line, and the coefficients (slope and intercept) have a straightforward meaning. The interpretation of the coefficients is clear: the slope represents the change in Y for a one-unit change in X.\n",
    "   \n",
    "   - **Polynomial Regression:**\n",
    "     - Polynomial regression can be harder to interpret because it fits a curve with multiple coefficients (for X, X², X³, etc.), and the coefficients of higher-degree terms may not have a simple interpretation. Additionally, interpreting the relationship between X and Y is more complex, as it is no longer a simple linear change.\n",
    "\n",
    "### 7. **Use Cases:**\n",
    "   - **Linear Regression:**\n",
    "     - Used when there is a clear linear relationship between the dependent and independent variables.\n",
    "     - Common in simple problems like predicting salary based on years of experience, or predicting price based on size.\n",
    "   \n",
    "   - **Polynomial Regression:**\n",
    "     - Used when the relationship between the dependent and independent variables is non-linear, such as when data follows a quadratic, cubic, or higher-order curve.\n",
    "     - Common in more complex problems, such as modeling growth patterns, modeling trajectories, or fitting data with turning points (e.g., U-shaped curves).\n",
    "\n",
    "### Conclusion:\n",
    "While **Linear Regression** is ideal for modeling simple linear relationships with one or more predictors, **Polynomial Regression** is used when the relationship is non-linear and requires the inclusion of polynomial terms to better capture the complexity of the data. Polynomial regression allows for greater flexibility in fitting data but introduces the risks of overfitting and reduced interpretability, particularly when using high-degree polynomials. Careful consideration of the data and the relationship being modeled is essential when choosing between linear and polynomial regression.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a642e1e-4a17-4a45-abac-5ccc591f1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#25.When is polynomial regression used\n",
    "'''\n",
    "Polynomial Regression is used in cases where the relationship between the independent and dependent variables is not linear, but still follows a \n",
    "recognizable pattern that can be captured by a polynomial function. Polynomial regression is suitable when a straight line (as in simple linear\n",
    "regression) does not adequately model the data, and a curve is needed for better fit and prediction.\n",
    "\n",
    "### Common Scenarios for Using Polynomial Regression:\n",
    "\n",
    "1. **Non-linear Relationships:**\n",
    "   - Polynomial regression is ideal when there is a **non-linear relationship** between the independent and dependent variables. In these cases, a \n",
    "   straight line won't capture the underlying pattern in the data, and polynomial terms (like X², X³, etc.) are added to fit a curved relationship.\n",
    "   - **Example:**\n",
    "     - Modeling the growth of a population or the spread of a disease, where the rate of change accelerates or decelerates over time.\n",
    "\n",
    "2. **Curved Trends in Data:**\n",
    "   - When the data exhibits a **curved trend** (such as parabolic, cubic, or other polynomial shapes), polynomial regression can better fit the data \n",
    "   and provide a more accurate model.\n",
    "   - **Example:**\n",
    "     - A U-shaped or inverted U-shaped curve in data, such as the relationship between the price of a product and its demand (demand increases initially,\n",
    "     then decreases after a certain point).\n",
    "\n",
    "3. **Modeling Data with Turning Points:**\n",
    "   - Polynomial regression is useful when the data exhibits **turning points** or inflection points, where the slope of the curve changes direction. \n",
    "   These turning points cannot be captured by a simple linear model but can be modeled using polynomial regression.\n",
    "   - **Example:**\n",
    "     - The trajectory of an object under the influence of gravity (projectile motion), where the path follows a quadratic or cubic curve.\n",
    "\n",
    "4. **Improving Model Fit in Complex Systems:**\n",
    "   - In complex systems where multiple factors contribute to an outcome in a non-linear fashion, polynomial regression helps in providing a **better \n",
    "   fit** to the data. This is particularly true when the relationships between variables are unknown or complex.\n",
    "   - **Example:**\n",
    "     - In economic modeling where relationships between various economic indicators (e.g., inflation, interest rates, and GDP growth) might be \n",
    "     non-linear.\n",
    "\n",
    "5. **Capturing Data Trends with Higher-order Effects:**\n",
    "   - When higher-order effects (such as the interaction between variables or the impact of squared or cubed terms) are important, polynomial regression\n",
    "   is used to include these effects in the model.\n",
    "   - **Example:**\n",
    "     - In machine learning tasks, where complex feature interactions need to be captured, polynomial regression allows for the inclusion of polynomial\n",
    "     terms to model these interactions.\n",
    "\n",
    "6. **Curve Fitting in Scientific and Engineering Applications:**\n",
    "   - In many scientific or engineering contexts, data often follows a non-linear curve, and polynomial regression is used to fit these curves to make \n",
    "   predictions or analyze the underlying relationship.\n",
    "   - **Example:**\n",
    "     - Fitting a curve to experimental data, such as the relationship between temperature and pressure in a chemical reaction or the relationship\n",
    "     between speed and fuel consumption in a vehicle.\n",
    "\n",
    "### When Not to Use Polynomial Regression:\n",
    "- **Overfitting Risk:** Polynomial regression is susceptible to overfitting, especially with higher-degree polynomials. Using high-degree polynomials\n",
    "may lead to a model that fits the training data well but performs poorly on unseen test data.\n",
    "- **Linear Relationship:** If the data follows a simple linear relationship, using polynomial regression is unnecessary and might introduce unnecessary\n",
    "complexity to the model.\n",
    "- **Extrapolation Issues:** Polynomial regression may not generalize well for extrapolation beyond the range of the training data. The model can produce\n",
    "unrealistic predictions when applied to new data points outside the trained range.\n",
    "\n",
    "### Conclusion:\n",
    "Polynomial regression is used when the relationship between the independent and dependent variables is non-linear, and a more flexible model is needed\n",
    "to capture complex patterns and trends in the data. It is suitable for problems with curved or turning-point relationships but should be used carefully \n",
    "to avoid overfitting and ensure the model generalizes well.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f4a9d1-9818-463e-a6e0-7120adcd0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#26.What is the general equation for polynomial regression\n",
    "'''\n",
    "The general equation for Polynomial Regression extends the equation of Linear Regression by including polynomial terms (such as X², X³, ...) to capture non-linear relationships between the dependent and independent variables.\n",
    "\n",
    "### General Polynomial Regression Equation:\n",
    "For a polynomial of degree **n**, the equation is:\n",
    "\n",
    "**Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βnXⁿ + ε**\n",
    "\n",
    "Where:\n",
    "- **Y** is the dependent variable (the output or prediction you want to model).\n",
    "- **X** is the independent variable (the input feature or predictor).\n",
    "- **β₀** is the intercept term, representing the value of Y when X = 0.\n",
    "- **β₁, β₂, ..., βn** are the coefficients (weights) that are learned by the model, each representing the influence of the corresponding polynomial term (X, X², X³, etc.) on the dependent variable Y.\n",
    "- **X², X³, ..., Xⁿ** are the polynomial terms, where X is raised to powers greater than 1 (squared, cubed, etc.).\n",
    "- **ε** is the error term (or residual), representing the difference between the actual and predicted values.\n",
    "\n",
    "### Explanation:\n",
    "- **Degree of the Polynomial (n):** \n",
    "  - The degree of the polynomial (n) determines the complexity of the curve. A degree of 1 represents a linear relationship (same as linear regression), degree 2 represents a quadratic relationship (a parabola), and higher degrees represent more complex curves (cubic, quartic, etc.).\n",
    "  \n",
    "  - **Degree 1 (Linear Regression):** \n",
    "    - **Y = β₀ + β₁X + ε**\n",
    "    - This is the simplest case, where the relationship between X and Y is linear.\n",
    "  \n",
    "  - **Degree 2 (Quadratic Regression):** \n",
    "    - **Y = β₀ + β₁X + β₂X² + ε**\n",
    "    - This is a parabolic relationship between X and Y.\n",
    "\n",
    "  - **Degree 3 (Cubic Regression):** \n",
    "    - **Y = β₀ + β₁X + β₂X² + β₃X³ + ε**\n",
    "    - This introduces more flexibility to model cubic (S-shaped) curves.\n",
    "\n",
    "  - **Higher Degrees:**\n",
    "    - As the degree increases (n > 3), the polynomial can fit even more complex curves. However, this comes with the risk of overfitting if the degree is too high.\n",
    "\n",
    "### Example:\n",
    "For a quadratic polynomial regression (degree 2), the equation might look like:\n",
    "**Y = β₀ + β₁X + β₂X² + ε**\n",
    "\n",
    "Here, β₀ is the intercept, β₁ is the coefficient for the linear term, and β₂ is the coefficient for the quadratic term. The model will fit a parabola to the data points.\n",
    "\n",
    "### Conclusion:\n",
    "The general equation for polynomial regression allows for capturing complex, non-linear relationships by including polynomial terms of the independent variable. The degree of the polynomial determines the complexity of the curve, with higher-degree polynomials offering more flexibility but also increasing the risk of overfitting.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42139ecc-6220-44ce-b113-7e5893719ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#27 Can polynomial regression be applied to multiple variables\n",
    "'''\n",
    "Yes, **Polynomial Regression** can be applied to multiple variables, and this is referred to as **Multiple Polynomial Regression**.\n",
    "In this case, the polynomial model includes not only powers of the individual variables but also interactions between multiple variables.\n",
    "\n",
    "### Key Points:\n",
    "1. **Polynomial Regression with One Variable (Simple Polynomial Regression):**\n",
    "   - For a single independent variable **X**, the polynomial regression model is of the form:\n",
    "     - **Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βnXⁿ + ε**\n",
    "   - This is known as **simple polynomial regression**, where the relationship between Y and X is modeled as a polynomial function.\n",
    "\n",
    "2. **Polynomial Regression with Multiple Variables (Multiple Polynomial Regression):**\n",
    "   - In **Multiple Polynomial Regression**, the independent variables (X₁, X₂, ..., Xp) can each have their own polynomial terms, and interaction terms\n",
    "   between the variables can also be included.\n",
    "   - The general equation for multiple polynomial regression (with interaction terms) for **p** variables is:\n",
    "     - **Y = β₀ + β₁X₁ + β₂X₂ + β₃X₁² + β₄X₂² + β₅X₁X₂ + ... + βnX₁ⁿ + ε**\n",
    "   \n",
    "     - Here:\n",
    "       - **Y** is the dependent variable.\n",
    "       - **X₁, X₂, ..., Xp** are the independent variables.\n",
    "       - **β₀** is the intercept.\n",
    "       - **β₁, β₂, ..., βn** are the coefficients of the polynomial terms and interaction terms.\n",
    "       - **ε** is the error term.\n",
    "       - **X₁², X₂², X₁X₂, X₁³, ...** are the polynomial and interaction terms.\n",
    "\n",
    "### How It Works:\n",
    "- **Polynomial Terms:**\n",
    "  - Just like in simple polynomial regression, the independent variables are raised to powers (e.g., X₁², X₂³) to model non-linear relationships \n",
    "  between the variables and the dependent variable.\n",
    "  \n",
    "- **Interaction Terms:**\n",
    "  - Interaction terms, like **X₁X₂**, allow the model to capture the effects of combinations of variables. This helps when the effect of one variable\n",
    "  on the dependent variable depends on the value of another variable.\n",
    "  \n",
    "  - **Example:** If you're modeling the relationship between **price** and both **size** and **age** of a product, the effect of size on price might \n",
    "  depend on the age of the product. In such cases, an interaction term like **size × age** can be included to model this relationship.\n",
    "\n",
    "- **Higher-Order Terms:**\n",
    "  - Higher-degree polynomials (like X₁³, X₂³, ...) can also be included to capture more complex relationships.\n",
    "\n",
    "### Example of Multiple Polynomial Regression Equation:\n",
    "For two independent variables **X₁** and **X₂** with quadratic and interaction terms, the polynomial regression equation could be:\n",
    "**Y = β₀ + β₁X₁ + β₂X₂ + β₃X₁² + β₄X₂² + β₅X₁X₂ + ε**\n",
    "\n",
    "### Applications:\n",
    "- **Interaction Effects:** Polynomial regression with multiple variables is especially useful in cases where you believe that the relationship between \n",
    "the dependent and independent variables is influenced not just by each variable individually, but also by interactions between them.\n",
    "- **Complex Curves:** When the relationship between multiple variables and the dependent variable is non-linear and involves higher-order terms or\n",
    "interactions, multiple polynomial regression can be used to model these complex relationships.\n",
    "- **Example:** Modeling the effect of both **temperature** and **humidity** on **crop yield**, where both variables and their interaction may have a\n",
    "non-linear effect on yield.\n",
    "\n",
    "### Conclusion:\n",
    "Polynomial regression can be extended to multiple variables, creating a more complex model that captures not just the individual effects of each\n",
    "variable but also the interactions between them. This is useful for modeling multi-dimensional non-linear relationships, but care must be taken to \n",
    "avoid overfitting, especially as more polynomial terms and interaction terms are added.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884aa2c-7aa5-4117-8583-1ff38ef8d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#28.What are the limitations of polynomial regression\n",
    "'''\n",
    "While Polynomial Regression is a powerful tool for modeling non-linear relationships, it has several limitations that need to be \n",
    "considered before applying it to a dataset.\n",
    "\n",
    "### 1. **Overfitting:**\n",
    "   - **Risk of Overfitting:** Polynomial regression, especially with high-degree polynomials, can lead to **overfitting**, where the model \n",
    "   fits the training data too closely. The model may capture not only the underlying trend but also the noise in the data, leading to poor \n",
    "   generalization to new, unseen data.\n",
    "   - **Example:** A high-degree polynomial may produce a model that fits the training data perfectly, but performs poorly on the test data, \n",
    "   as it is too complex and sensitive to minor fluctuations in the data.\n",
    "\n",
    "### 2. **Difficulty in Interpretation:**\n",
    "   - **Complexity of the Model:** As the degree of the polynomial increases, the model becomes more complex and harder to interpret. Each additional \n",
    "   polynomial term adds a new layer of complexity, making it difficult to understand the relationship between the independent variables and the \n",
    "   dependent variable.\n",
    "   - **Example:** In a cubic or higher-degree polynomial, the influence of each individual term (e.g., X², X³, etc.) on the outcome may not be easily \n",
    "   interpretable.\n",
    "\n",
    "### 3. **Extrapolation Issues:**\n",
    "   - **Extrapolation Risks:** Polynomial regression is prone to **extrapolation issues**, particularly when predicting values outside the range of the\n",
    "   training data. A high-degree polynomial may produce unrealistic predictions as it is highly sensitive to input values that are far from the data used\n",
    "   to fit the model.\n",
    "   - **Example:** Using a high-degree polynomial to predict outcomes for values of X that were not observed in the training set could lead to wildly \n",
    "   inaccurate predictions, especially when the polynomial curve oscillates drastically outside the training data range.\n",
    "\n",
    "### 4. **Computational Complexity:**\n",
    "   - **Increased Computation for High-Degree Polynomials:** As the degree of the polynomial increases, the number of terms in the equation grows, \n",
    "   leading to higher computational costs. This can make training the model slower, especially for large datasets.\n",
    "   - **Example:** For large datasets with high-degree polynomials, the computational time and resources required to fit the model and make predictions\n",
    "   can increase significantly.\n",
    "\n",
    "### 5. **Multicollinearity:**\n",
    "   - **Multicollinearity Among Polynomial Terms:** When including higher-degree polynomial terms (e.g., X², X³), the model can suffer from\n",
    "   **multicollinearity**, where the predictor variables (including the polynomial terms) become highly correlated. This makes it difficult \n",
    "   to estimate the individual coefficients reliably and can lead to unstable model predictions.\n",
    "   - **Example:** If X and X² are both included as predictors, the correlation between X and X² can create problems in estimating the relationship\n",
    "   between these terms and the dependent variable.\n",
    "\n",
    "### 6. **Choosing the Right Degree:**\n",
    "   - **Difficulty in Selecting the Optimal Degree:** Selecting the appropriate degree for the polynomial is not always straightforward. A low-degree\n",
    "   polynomial might underfit the data, while a high-degree polynomial might overfit it. Determining the optimal degree often requires trial and error, \n",
    "   cross-validation, or other techniques like **regularization**.\n",
    "   - **Example:** Trying different polynomial degrees and assessing the model’s performance on a validation set can help determine the best degree, but\n",
    "   this can be time-consuming and computationally expensive.\n",
    "\n",
    "### 7. **Inability to Handle Complex Interactions:**\n",
    "   - **Limited to Polynomial Interactions:** While polynomial regression can capture some types of non-linear relationships, it is limited to polynomial\n",
    "   terms and cannot model more complex interactions between variables that are not polynomial in nature (e.g., interactions that require other \n",
    "   transformations or non-polynomial functions).\n",
    "   - **Example:** In real-world data, some relationships may require exponential or logarithmic transformations, which polynomial regression cannot\n",
    "   capture as effectively.\n",
    "\n",
    "### 8. **Assumption of Smoothness:**\n",
    "   - **Smooth Curves:** Polynomial regression assumes that the relationship between the independent and dependent variables is **smooth**. However, \n",
    "   real-world data may have abrupt changes or discontinuities that a polynomial curve cannot model accurately.\n",
    "   - **Example:** A sudden change in the relationship between X and Y, like a step function, cannot be effectively captured by polynomial regression.\n",
    "\n",
    "### Conclusion:\n",
    "Polynomial regression is a useful tool for capturing non-linear relationships between variables, but it has significant limitations, such as overfitting,\n",
    "computational complexity, and difficulties with extrapolation and interpretation. These limitations can be mitigated by using techniques such as\n",
    "**regularization** (e.g., Ridge or Lasso regression), selecting the appropriate degree for the polynomial, and carefully evaluating the model’s \n",
    "performance on unseen data. It’s also important to consider alternative models, such as decision trees or neural networks, when polynomial regression \n",
    "is not suitable for the problem at hand.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5da58-1804-4b94-88a1-ff440a18b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#29.what methods can be used to evaluate model fit when selecting the degree of a polynomial.\n",
    "'''\n",
    "When selecting the degree of a polynomial for polynomial regression, it is essential to evaluate how well the model fits the data without overfitting.\n",
    "Several methods can be used to evaluate model fit and help in choosing the optimal degree of the polynomial:\n",
    "\n",
    "### 1. **Visual Inspection of the Model Fit:**\n",
    "   - **Plotting the data and the model:** One of the simplest ways to evaluate the fit of a polynomial model is to visually inspect how well the \n",
    "   polynomial curve fits the observed data.\n",
    "     - **Training Data:** Plot the data points and overlay the polynomial regression curve for different degrees. Visually, you can observe if the curve \n",
    "     seems to follow the underlying trend of the data or if it starts to oscillate excessively (overfitting).\n",
    "     - **Test Data:** It is also useful to plot the model’s predictions for unseen data (test data) to see how well the polynomial curve generalizes.\n",
    "     - **Recommendation:** Start with lower-degree polynomials and incrementally increase the degree, visually inspecting if the model starts \n",
    "     overfitting (waving excessively through the data) as the degree increases.\n",
    "\n",
    "### 2. **Cross-Validation:**\n",
    "   - **K-fold Cross-Validation:** Cross-validation helps in assessing the performance of the polynomial regression model on unseen data. By dividing the\n",
    "   \n",
    "   data into K subsets (folds), the model is trained on K-1 folds and tested on the remaining fold. This process is repeated K times, and the average \n",
    "   performance metric (like RMSE or MAE) is computed.\n",
    "     - Cross-validation can be used to evaluate how well the polynomial model generalizes to new data, which helps in determining the appropriate degree\n",
    "     of the polynomial.\n",
    "     - **Recommendation:** Use K-fold cross-validation with different polynomial degrees to check which degree gives the best performance on the\n",
    "     validation data, avoiding overfitting to the training data.\n",
    "\n",
    "### 3. **Adjusted R²:**\n",
    "   - **Adjusted R²** is an improvement over the regular R² (coefficient of determination) when evaluating polynomial regression models. While R² \n",
    "   always increases as more polynomial terms are added, **Adjusted R²** takes into account the number of predictors and adjusts for overfitting. \n",
    "   It penalizes the addition of irrelevant features (higher-degree terms) that do not significantly improve the model.\n",
    "     - **Formula:** \n",
    "       - **Adjusted R² = 1 - [(1 - R²)(n - 1) / (n - p - 1)]**\n",
    "       - Where:\n",
    "         - **n** is the number of data points.\n",
    "         - **p** is the number of predictors (degree of the polynomial).\n",
    "     - **Recommendation:** A higher Adjusted R² indicates a better fit with fewer predictors. Compare Adjusted R² for different polynomial degrees to\n",
    "     find the degree that provides the best fit without overfitting.\n",
    "\n",
    "### 4. **Root Mean Squared Error (RMSE) or Mean Absolute Error (MAE):**\n",
    "   - **RMSE** and **MAE** are both metrics used to evaluate the accuracy of the model's predictions. Lower values of RMSE or MAE indicate better model\n",
    "   fit.\n",
    "     - **RMSE** gives a higher penalty to large errors due to its squared nature.\n",
    "     - **MAE** gives a linear penalty to errors, which might be preferred in some contexts.\n",
    "     - **Recommendation:** Calculate RMSE or MAE for different degrees of the polynomial, and choose the degree that minimizes these metrics. If the \n",
    "     error starts increasing with higher polynomial degrees, it could indicate overfitting.\n",
    "\n",
    "### 5. **Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC):**\n",
    "   - **AIC and BIC** are statistical criteria that help in model selection, balancing goodness-of-fit with model complexity. Both penalize the\n",
    "   inclusion of unnecessary predictors, thus preventing overfitting.\n",
    "     - **AIC Formula:** \n",
    "       - **AIC = 2k - 2ln(L)**\n",
    "       - Where:\n",
    "         - **k** is the number of parameters (polynomial degree).\n",
    "         - **L** is the likelihood of the model.\n",
    "     - **BIC Formula:**\n",
    "       - **BIC = ln(n)k - 2ln(L)**\n",
    "       - Where:\n",
    "         - **n** is the number of observations.\n",
    "         - **k** is the number of parameters.\n",
    "     - **Recommendation:** Lower AIC or BIC values indicate a better model. Use AIC/BIC to compare models with different polynomial degrees and choose \n",
    "     the one with the lowest value.\n",
    "\n",
    "### 6. **Learning Curves:**\n",
    "   - **Learning curves** plot the training error and validation error as a function of the training set size or the number of training iterations. By \n",
    "   comparing the errors for different polynomial degrees, you can identify which degree leads to the best generalization.\n",
    "     - If the training error is low but the validation error is high, the model may be overfitting.\n",
    "     - If both errors are high, the model may be underfitting.\n",
    "     - **Recommendation:** Use learning curves to check if increasing the polynomial degree leads to a lower validation error, without causing excessive\n",
    "     overfitting.\n",
    "\n",
    "### 7. **Training vs. Test Set Performance:**\n",
    "   - **Performance comparison:** After training the polynomial regression model on the training data, evaluate the model on both the training and test \n",
    "   sets. If the model performs well on the training data but poorly on the test data (high training error and low test error), it could indicate\n",
    "   overfitting.\n",
    "     - **Recommendation:** Choose the polynomial degree that provides a balance between training and test set performance, ensuring the model \n",
    "     generalizes well to unseen data.\n",
    "\n",
    "### Conclusion:\n",
    "When selecting the degree of a polynomial for polynomial regression, it is crucial to balance model complexity with generalization ability. Methods \n",
    "such as visual inspection, cross-validation, Adjusted R², RMSE, AIC/BIC, learning curves, and performance on training/test sets help assess how well\n",
    "the model fits the data. By using these methods, you can select the optimal polynomial degree that minimizes overfitting and provides the best model\n",
    "performance.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc48b347-1250-43c8-bf43-5bb69649d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#30.Why is visualization important in polynomial regression\n",
    "'''\n",
    "Visualization plays a critical role in understanding and interpreting the results of **polynomial regression**. It helps in diagnosing potential \n",
    "issues with the model, identifying patterns, and ensuring the model is fitting the data correctly. Below are several reasons why visualization is\n",
    "particularly important in polynomial regression:\n",
    "\n",
    "### 1. **Understanding the Relationship Between Variables:**\n",
    "   - **Visualizing the Polynomial Curve:** Polynomial regression is often used when the relationship between the dependent and independent variables\n",
    "   is non-linear. Plotting the data and the fitted polynomial curve allows you to visually inspect how well the model captures the underlying trend in\n",
    "   the data.\n",
    "   - **Example:** If you're modeling the relationship between a variable like **age** and **income**, a scatter plot of the data along with the \n",
    "   polynomial regression curve will help you see whether a linear model is insufficient and a polynomial model fits the data better.\n",
    "\n",
    "### 2. **Detecting Overfitting:**\n",
    "   - **Overfitting Visualization:** One of the most common issues in polynomial regression is overfitting, especially when using higher-degree\n",
    "   polynomials. A high-degree polynomial might fit the training data perfectly but will not generalize well to unseen data.\n",
    "   - **Visual Warning Signs:** By visualizing both the training and test data along with the fitted polynomial curve, you can see if the curve starts \n",
    "   to oscillate wildly or fits noise in the data (which is a sign of overfitting).\n",
    "   - **Example:** If the curve closely follows the fluctuations of the training data but fails to predict the test data accurately, this suggests the \n",
    "   model may be overfitting.\n",
    "\n",
    "### 3. **Identifying the Appropriate Polynomial Degree:**\n",
    "   - **Choosing the Degree of the Polynomial:** Visualization can help you decide which degree of polynomial provides the best fit. Starting with a\n",
    "   low-degree polynomial and gradually increasing the degree allows you to see how the curve changes and whether it starts to fit the noise in the \n",
    "   data.\n",
    "   - **Example:** If increasing the degree of the polynomial doesn’t improve the fit or leads to excessive oscillations, it might indicate that a \n",
    "   higher-degree polynomial is unnecessary.\n",
    "\n",
    "### 4. **Diagnosing Model Assumptions:**\n",
    "   - **Checking Residuals:** After fitting a polynomial regression model, it is important to inspect the residuals (the differences between the observed \n",
    "   and predicted values). Visualization of residuals (via residual plots) helps in detecting patterns or systematic deviations, which might suggest that the polynomial degree is incorrect.\n",
    "   - **Example:** If the residuals show a pattern or curve, it may indicate that the polynomial model is not capturing the full complexity of the data,\n",
    "   and a different degree or model might be more appropriate.\n",
    "\n",
    "### 5. **Evaluating the Fit on Training and Test Data:**\n",
    "   - **Visualizing Training vs. Test Data Performance:** When assessing model performance, it’s important to visualize how well the polynomial model\n",
    "   performs on both the training data and the test data.\n",
    "   - **Example:** A good model will have a smooth curve that fits the training data well and generalizes to the test data. If the polynomial curve \n",
    "   fits the training data but performs poorly on the test data (especially if the curve is too complex), it may indicate overfitting.\n",
    "\n",
    "### 6. **Improving Model Interpretability:**\n",
    "   - **Interpretation of the Model's Behavior:** By plotting the fitted polynomial curve, you can better interpret the model’s behavior. Visualization \n",
    "   helps to understand how changes in the independent variable(s) affect the dependent variable.\n",
    "   - **Example:** In a quadratic model, you can visually identify the turning points or where the dependent variable increases or decreases based on\n",
    "   the independent variable's value.\n",
    "\n",
    "### 7. **Highlighting Key Features and Trends:**\n",
    "   - **Revealing Patterns in Data:** Visualization helps uncover trends, patterns, or specific features in the data that may not be apparent from raw \n",
    "   numbers. It can indicate whether the polynomial regression model is capturing the true underlying structure of the data or if it is missing important features.\n",
    "   - **Example:** In a cubic polynomial regression, the curve might capture multiple peaks and valleys, revealing insights about complex relationships \n",
    "   between variables.\n",
    "\n",
    "### 8. **Facilitating Communication and Presentation:**\n",
    "   - **Explaining Results to Stakeholders:** Visualization is an effective way to communicate the results of polynomial regression to non-technical \n",
    "   stakeholders. A clear plot of the data with the fitted polynomial curve can make the model's performance and predictions easier to understand.\n",
    "   - **Example:** When presenting findings to a client or manager, showing a visual of the polynomial regression curve along with data points helps \n",
    "   them understand the relationship you are modeling and the significance of the polynomial terms.\n",
    "\n",
    "### Conclusion:\n",
    "Visualization is a powerful tool in polynomial regression as it helps assess the fit, identify potential issues like overfitting, and facilitate the\n",
    "selection of the appropriate polynomial degree. By visualizing the data, the fitted model, residuals, and the performance on both training and test \n",
    "data, you gain deeper insights into how well the model represents the underlying relationship. Additionally, it aids in presenting the results clearly \n",
    "to others and ensuring that the model generalizes well to unseen data.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b343568-979d-4960-af58-2d2e50c46ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471482f7-be45-42aa-920c-bfb85aab0ad0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m10\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshaping to make X a 2D array\u001b[39;00m\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m15000\u001b[39m, \u001b[38;5;241m20000\u001b[39m, \u001b[38;5;241m25000\u001b[39m, \u001b[38;5;241m30000\u001b[39m, \u001b[38;5;241m40000\u001b[39m, \u001b[38;5;241m55000\u001b[39m, \u001b[38;5;241m70000\u001b[39m, \u001b[38;5;241m90000\u001b[39m, \u001b[38;5;241m120000\u001b[39m, \u001b[38;5;241m150000\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a712b0de-5130-4a96-851f-d99ef8a3b4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LinearRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#31.How is polynomial regression implemented in Python\n",
    "'''\n",
    "Polynomial Regression is implemented in Python using libraries like **NumPy** and **scikit-learn**. \n",
    "Here is a step-by-step guide on how to implement Polynomial Regression in Python:\n",
    "\n",
    "### 1. **Import Required Libraries:**\n",
    "To start, you'll need to import the necessary libraries for polynomial regression.\n",
    "\n",
    "\n",
    "# Importing necessary libraries\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "'''\n",
    "2. Prepare the Data:\n",
    "You’ll need a dataset to apply polynomial regression. For illustration, let’s generate a sample dataset or use an existing one.\n",
    "\n",
    "Edit\n",
    "# Example dataset\n",
    "# X = independent variable (e.g., years of experience)\n",
    "# y = dependent variable (e.g., salary)\n",
    "'''\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)  # Reshaping to make X a 2D array\n",
    "y = np.array([15000, 20000, 25000, 30000, 40000, 55000, 70000, 90000, 120000, 150000])\n",
    "'''\n",
    "3. Create Polynomial Features:\n",
    "Polynomial regression requires transforming the original features (X) into higher-degree features (e.g., X², X³, etc.). \n",
    "This is done using the PolynomialFeatures class from sklearn.\n",
    "'''\n",
    "# Choose the degree of the polynomial\n",
    "degree = 3\n",
    "\n",
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "# Transform the data into polynomial features\n",
    "X_poly = poly.fit_transform(X)\n",
    "'''\n",
    "4. Fit the Polynomial Regression Model:\n",
    "Now, you can use the transformed data to fit a linear regression model. The LinearRegression model will fit the polynomial terms.\n",
    "\\\n",
    "# Fitting Polynomial Regression to the dataset\n",
    "'''\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "\n",
    "'''\n",
    "5. Make Predictions:\n",
    "Once the model is trained, you can make predictions using the polynomial features of new data points.\n",
    "\n",
    "\n",
    "# Making predictions for the same dataset (or new data)\n",
    "'''\n",
    "y_pred = lin_reg.predict(X_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2553bb90-75f6-40cc-a9ae-cbf3d1955b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = lin_reg.predict(X_poly)\n",
    "'''\n",
    "6. Visualizing the Polynomial Regression Results:\n",
    "Visualization helps you understand how well the polynomial regression model fits the data.\n",
    "We will plot both the original data points and the polynomial regression curve.\n",
    "'''\n",
    "# Visualizing the Polynomial Regression results\n",
    "plt.scatter(X, y, color='red')  # Plotting the original data\n",
    "plt.plot(X, y_pred, color='blue')  # Plotting the polynomial regression line\n",
    "plt.title('Polynomial Regression')\n",
    "plt.xlabel('X (e.g., years of experience)')\n",
    "plt.ylabel('y (e.g., salary)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d93d9fb-1dbb-4742-93ed-07fd3a6fc048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AYUSH\\AppData\\Local\\Temp\\ipykernel_10588\\3417674596.py:4: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  X_grid = np.arange(min(X), max(X), 0.1)  # Create a smooth grid of X values\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''7. Optional: Visualizing the Polynomial Curve for Higher Resolution:\n",
    "To make the curve smoother, you can plot the polynomial curve with more points by using a higher resolution grid for X.'''\n",
    "# Visualizing the Polynomial Regression results with higher resolution\n",
    "X_grid = np.arange(min(X), max(X), 0.1)  # Create a smooth grid of X values\n",
    "X_grid = X_grid.reshape((len(X_grid), 1))  # Reshaping for compatibility\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(X, y, color='red')\n",
    "plt.plot(X_grid, lin_reg.predict(poly.transform(X_grid)), color='blue')\n",
    "plt.title('Polynomial Regression (Higher Resolution)')\n",
    "plt.xlabel('X (e.g., years of experience)')\n",
    "plt.ylabel('y (e.g., salary)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f217b8-8ed2-4a6c-b671-40915895b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Conclusion:\n",
    "In summary, polynomial regression in Python is implemented using NumPy, matplotlib, and scikit-learn. \n",
    "The process involves transforming the input data into polynomial features, fitting a linear regression model\n",
    "to the transformed data, and visualizing the results to evaluate the model fit. This approach allows you to capture\n",
    "non-linear relationships between the independent and dependent variables while leveraging linear regression techniques. '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
